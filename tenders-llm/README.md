# LLM Tender Prediction (Title + Text)

End-to-end pipeline to classify tenders using OpenAI LLMs with few-shot learning, text summarization, and PR-AUC evaluation.

## Quick Runbook

### 1) Put your data in `data/raw/`

- **`tenders.csv`** with columns:
  - `id` (required)
  - `title` (required)
  - `full_text` (required)
  - `date` (optional, ISO-8601 for time-based splits)
  - `source` (optional)

- **`selected_ids.csv`**: one tender ID per line (the ~170 human-selected positives)

- **`services.md`**: client service descriptions (plain text/markdown)

- **`keywords.csv`** (optional) with columns:
  - `keyword` (required)
  - `lang` (optional: 'en', 'de', 'fr', etc.)
  - `weight` (optional: importance)
  - `category` (optional: grouping for prompt readability)

### 2) Install

```bash
python -m venv .venv && source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### 3) Run pipeline

```bash
cd tenders-llm

python scripts/01_prepare_data.py   # clean text, detect language, create labels
python scripts/02_make_splits.py    # train/val/test split (75/10/15)
python scripts/03_build_prompt.py   # inject services, keywords, few-shot examples
python scripts/04_llm_predict.py    # classify val/test/all with GPT-4o-mini
python scripts/05_eval.py           # compute PR-AUC, precision@K, save curves
```

### 4) Check results

- **Predictions**: `data/processed/preds_val.jsonl`, `preds_test.jsonl`, `preds_all.jsonl`
- **Metrics**: `reports/pr_curve_val.png`, `pr_curve_test.png`
- **Top-K analysis**: printed to console (precision@K, recall@K for K=50,100,170,200,300)

## Pipeline Steps

| Step | Script | What it does |
|------|--------|--------------|
| 1 | `01_prepare_data.py` | Load raw CSVs, clean text with NFKC normalization, detect language, create binary labels from `selected_ids.csv` |
| 2 | `02_make_splits.py` | Time-based split (if dates available) or stratified random split (75% train, 10% val, 15% test) |
| 3 | `03_build_prompt.py` | Build LLM prompt by injecting services, keywords, and up to 100 few-shot positive examples from training set |
| 4 | `04_llm_predict.py` | Batch classify val/test/all sets; summarize long texts; output JSONL with prediction, confidence, reasoning |
| 5 | `05_eval.py` | Compute PR-AUC, precision@K, recall@K; save PR curves to `reports/` |

## Configuration

Edit `.env`:
- `OPENAI_API_KEY`: your OpenAI API key
- `OPENAI_MODEL`: model for classification (default: `gpt-4o-mini`)
- `OPENAI_SUMMARY_MODEL`: model for summarization (default: same as above)
- `TEMPERATURE`: sampling temperature (default: 0.1)
- `RANDOM_SEED`: reproducibility seed (default: 42)

## Project Structure

```
tenders-llm/
  data/
    raw/               # input data (CSV, MD, etc.)
    processed/         # cleaned data, splits, predictions
  prompts/
    classify_tender.md # LLM prompt (auto-generated by step 3)
  reports/             # PR curves, top-K metrics
  scripts/             # 01–05 pipeline steps
  utils/
    text_clean.py      # NFKC normalization, zero-width removal
  .env                 # config (not in git)
  requirements.txt
```

## Notes

- Uses OpenAI structured output (`response_format={"type":"json_object"}`) for reliable parsing.
- Long texts (>1200 chars) are auto-summarized to ≤200 words before classification.
- Few-shot examples (positive titles from train set) improve precision on domain-specific tenders.
- For cost control: adjust `MAX_WORKERS` in `.env` or batch API calls; cache embeddings if repeating.

