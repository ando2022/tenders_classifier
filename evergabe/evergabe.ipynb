{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55bacc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 3.1/4.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 16.8 MB/s  0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c71a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from webdriver-manager) (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\damla\\appdata\\roaming\\python\\python313\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->webdriver-manager) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6da44c",
   "metadata": {},
   "source": [
    "BELOW THERE IS \"EXTRAFILTERED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e91987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ ~127 pages, page size 10\n",
      "📄 Page 1 … kept 4\n",
      "📄 Page 2 … kept 2\n",
      "📄 Page 3 … kept 2\n",
      "📄 Page 4 … kept 2\n",
      "📄 Page 5 … kept 1\n",
      "📄 Page 6 … kept 1\n",
      "📄 Page 7 … kept 2\n",
      "📄 Page 8 … kept 2\n",
      "📄 Page 9 … kept 2\n",
      "📄 Page 10 … kept 1\n",
      "📄 Page 11 … kept 2\n",
      "📄 Page 12 … kept 0\n",
      "📄 Page 13 … kept 2\n",
      "📄 Page 14 … kept 2\n",
      "📄 Page 15 … kept 2\n",
      "📄 Page 16 … kept 5\n",
      "📄 Page 17 … kept 4\n",
      "📄 Page 18 … kept 5\n",
      "📄 Page 19 … kept 1\n",
      "📄 Page 20 … kept 2\n",
      "📄 Page 21 … kept 4\n",
      "📄 Page 22 … kept 1\n",
      "📄 Page 23 … kept 3\n",
      "📄 Page 24 … kept 2\n",
      "📄 Page 25 … kept 3\n",
      "📄 Page 26 … kept 1\n",
      "📄 Page 27 … kept 2\n",
      "📄 Page 28 … kept 2\n",
      "📄 Page 29 … kept 1\n",
      "📄 Page 30 … kept 1\n",
      "📄 Page 31 … kept 0\n",
      "📄 Page 32 … kept 4\n",
      "📄 Page 33 … kept 3\n",
      "📄 Page 34 … kept 1\n",
      "📄 Page 35 … kept 2\n",
      "📄 Page 36 … kept 1\n",
      "📄 Page 37 … kept 1\n",
      "📄 Page 38 … kept 2\n",
      "📄 Page 39 … kept 2\n",
      "📄 Page 40 … kept 3\n",
      "📄 Page 41 … kept 3\n",
      "📄 Page 42 … kept 1\n",
      "📄 Page 43 … kept 2\n",
      "📄 Page 44 … kept 2\n",
      "📄 Page 45 … kept 2\n",
      "📄 Page 46 … kept 4\n",
      "📄 Page 47 … kept 0\n",
      "📄 Page 48 … kept 2\n",
      "📄 Page 49 … kept 0\n",
      "📄 Page 50 … kept 2\n",
      "📄 Page 51 … kept 1\n",
      "📄 Page 52 … kept 4\n",
      "📄 Page 53 … kept 1\n",
      "📄 Page 54 … kept 1\n",
      "📄 Page 55 … kept 1\n",
      "📄 Page 56 … kept 3\n",
      "📄 Page 57 … kept 0\n",
      "📄 Page 58 … kept 4\n",
      "📄 Page 59 … kept 4\n",
      "📄 Page 60 … kept 0\n",
      "📄 Page 61 … kept 3\n",
      "📄 Page 62 … kept 1\n",
      "📄 Page 63 … kept 1\n",
      "📄 Page 64 … kept 3\n",
      "📄 Page 65 … kept 3\n",
      "📄 Page 66 … kept 2\n",
      "📄 Page 67 … kept 3\n",
      "📄 Page 68 … kept 5\n",
      "📄 Page 69 … kept 1\n",
      "📄 Page 70 … kept 3\n",
      "📄 Page 71 … kept 1\n",
      "📄 Page 72 … kept 1\n",
      "📄 Page 73 … kept 1\n",
      "📄 Page 74 … kept 2\n",
      "📄 Page 75 … kept 1\n",
      "📄 Page 76 … kept 0\n",
      "📄 Page 77 … kept 3\n",
      "📄 Page 78 … kept 1\n",
      "📄 Page 79 … kept 2\n",
      "📄 Page 80 … kept 1\n",
      "📄 Page 81 … kept 2\n",
      "📄 Page 82 … kept 2\n",
      "📄 Page 83 … kept 2\n",
      "📄 Page 84 … kept 2\n",
      "📄 Page 85 … kept 3\n",
      "📄 Page 86 … kept 1\n",
      "📄 Page 87 … kept 2\n",
      "📄 Page 88 … kept 2\n",
      "📄 Page 89 … kept 5\n",
      "📄 Page 90 … kept 2\n",
      "📄 Page 91 … kept 3\n",
      "📄 Page 92 … kept 1\n",
      "📄 Page 93 … kept 2\n",
      "📄 Page 94 … kept 2\n",
      "📄 Page 95 … kept 0\n",
      "📄 Page 96 … kept 2\n",
      "📄 Page 97 … kept 5\n",
      "📄 Page 98 … kept 2\n",
      "📄 Page 99 … kept 3\n",
      "📄 Page 100 … kept 5\n",
      "📄 Page 101 … kept 4\n",
      "📄 Page 102 … kept 1\n",
      "📄 Page 103 … kept 0\n",
      "📄 Page 104 … kept 2\n",
      "📄 Page 105 … kept 2\n",
      "📄 Page 106 … kept 2\n",
      "📄 Page 107 … kept 3\n",
      "📄 Page 108 … kept 3\n",
      "📄 Page 109 … kept 0\n",
      "📄 Page 110 … kept 1\n",
      "📄 Page 111 … kept 3\n",
      "📄 Page 112 … kept 2\n",
      "📄 Page 113 … kept 4\n",
      "📄 Page 114 … kept 1\n",
      "📄 Page 115 … kept 2\n",
      "📄 Page 116 … kept 3\n",
      "📄 Page 117 … kept 3\n",
      "📄 Page 118 … kept 0\n",
      "📄 Page 119 … kept 2\n",
      "📄 Page 120 … kept 6\n",
      "📄 Page 121 … kept 2\n",
      "📄 Page 122 … kept 0\n",
      "📄 Page 123 … kept 3\n",
      "📄 Page 124 … kept 0\n",
      "📄 Page 125 … kept 2\n",
      "📄 Page 126 … kept 4\n",
      "📄 Page 127 … kept 1\n",
      "✅ Saved 262 rows to evergabe_filtered_multilang_20251001_131841.csv\n"
     ]
    }
   ],
   "source": [
    "# evergabe_singlepass_incl_excl_multilang.py\n",
    "# FAST single-pass scraper with multilingual inclusion & exclusion filters,\n",
    "# full column extraction, and per-cell link capture.\n",
    "\n",
    "import csv, time, re, math, datetime, unicodedata\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "START_URL = \"https://www.evergabe-online.de/search.html?4\"\n",
    "\n",
    "# ---------------- INCLUSION KEYWORDS (DE/FR/IT/EN) ----------------\n",
    "RAW_INCLUDE = [\n",
    "    # Study / Studie\n",
    "    \"study\", \"studie\", \"étude\", \"etude\", \"studio\",\n",
    "\n",
    "    # Analysis / Analyse\n",
    "    \"analysis\", \"analyse\", \"analisi\",\n",
    "\n",
    "    # Economy / Wirtschaft / Ökonomie\n",
    "    \"economy\", \"economics\", \"wirtschaft\", \"ökonomie\", \"economie\", \"economia\",\n",
    "\n",
    "    # Benchmarking\n",
    "    \"benchmark\", \"benchmarking\",\n",
    "\n",
    "    # Wirtschaftsberatung (economic/business consulting)\n",
    "    \"wirtschaftsberatung\", \"consulting\", \"conseil\", \"consulenza\",\n",
    "\n",
    "    # CPV & categories\n",
    "    \"72000000\", \"79300000\",\n",
    "    \"it-dienste\",\n",
    "    \"beratung, software-entwicklung, internet und hilfestellung\",\n",
    "    \"markt- und wirtschaftsforschung\", \"umfragen und statistiken\",\n",
    "\n",
    "    # Dienstleistung\n",
    "    \"dienstleistung\", \"service\", \"prestation\", \"prestazione\",\n",
    "\n",
    "    # Ausschreibung\n",
    "    \"ausschreibung\", \"appel d'offres\", \"appels d'offres\", \"gara\", \"procurement\",\n",
    "\n",
    "    # Offenes Verfahren\n",
    "    \"offenes verfahren\", \"procédure ouverte\", \"procedure ouverte\", \"procedura aperta\",\n",
    "\n",
    "    # BKS / Bundesamt für Statistik / OFS\n",
    "    \"bks\",\n",
    "    \"bundesamt für statistik\",\n",
    "    \"office fédéral de la statistique\", \"office federal de la statistique\",\n",
    "    \"ufficio federale di statistica\",\n",
    "    \"federal statistical office\", \"ofs\", \"bfs\",\n",
    "\n",
    "    # SECO / Staatssekretariat für Wirtschaft\n",
    "    \"seco\", \"staatssekretariat für wirtschaft\",\n",
    "    \"secrétariat d'état à l'économie\", \"secretariat d'etat a l'economie\",\n",
    "    \"segretariato di stato dell'economia\",\n",
    "\n",
    "    # Cantons / regions (Zürich, Luzern, generic canton/region)\n",
    "    \"kanton zürich\", \"kanton zuerich\", \"canton de zurich\", \"cantone zurigo\",\n",
    "    \"kanton luzern\", \"canton de lucerne\", \"cantone lucerna\",\n",
    "    \"kanton\", \"canton\", \"cantone\",\n",
    "    \"region\", \"regionen\", \"région\", \"regione\",\n",
    "\n",
    "    # Bundesamt für… (generic)\n",
    "    \"bundesamt für\", \"bundesamt fuer\",\n",
    "\n",
    "    # BBL\n",
    "    \"bundesamt für bauten und logistik\", \"bundesamt fuer bauten und logistik\", \"bbl\",\n",
    "\n",
    "    # Index (e.g. Verbraucherindex)\n",
    "    \"index\", \"verbraucherindex\", \"price index\", \"indice des prix\", \"indice dei prezzi\",\n",
    "\n",
    "    # Wirtschaftsforschung\n",
    "    \"wirtschaftsforschung\", \"economic research\", \"recherche économique\",\n",
    "    \"recherche economique\", \"ricerca economica\",\n",
    "]\n",
    "\n",
    "# ---------------- EXCLUSION KEYWORDS (DE/FR/IT/EN) ----------------\n",
    "RAW_EXCLUDE = [\n",
    "    \"construction\", \"bau\", \"bâtiment\", \"batiment\", \"costruzioni\",\n",
    "    \"health care\", \"gesundheit\", \"santé\", \"sante\", \"sanità\", \"sanita\",\n",
    "    \"transport\", \"verkehr\", \"transports\", \"trasporti\",\n",
    "    \"mobility\", \"mobilität\", \"mobilite\", \"mobilité\", \"mobilita\",\n",
    "    \"sport\",  # same in most langs\n",
    "    \"culture\", \"kultur\", \"culture\", \"cultura\",\n",
    "    \"street\", \"strasse\", \"straße\", \"rue\", \"strada\",\n",
    "    \"infrastructure\", \"infrastruktur\", \"infrastructure\", \"infrastruttura\",\n",
    "    \"process\", \"prozess\", \"processus\", \"processo\",\n",
    "    \"it\", \"informatik\", \"informatique\", \"informatica\",\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"lowercase, strip accents, collapse spaces\"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "INCLUDE = {_norm(k) for k in RAW_INCLUDE if k.strip()}\n",
    "EXCLUDE = {_norm(k) for k in RAW_EXCLUDE if k.strip()}\n",
    "\n",
    "# ---------------- Selenium setup ----------------\n",
    "def make_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--lang=de-DE\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    labels = [\"Alle akzeptieren\",\"Akzeptieren\",\"Zustimmen\",\"Einverstanden\",\"OK\",\"Okay\",\"Ich stimme zu\"]\n",
    "    for label in labels:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//button[contains(., '{label}')]\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def wait_for_table(driver, timeout=20):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table tbody tr\"))\n",
    "    )\n",
    "\n",
    "def first_cell_token(driver):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"table tbody tr td\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def set_page_size_max(driver):\n",
    "    # Try <select> page-size\n",
    "    tried = False\n",
    "    for sel in [\"select\", \"select.page-size\", \"select[name*='size' i]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "            s = Select(el)\n",
    "            labels = [o.text.strip() or o.get_attribute(\"value\") for o in s.options]\n",
    "            target = None\n",
    "            for want in (\"100\",\"200\",\"Alle\",\"Alles\"):\n",
    "                for lbl in labels:\n",
    "                    if want in lbl:\n",
    "                        target = lbl; break\n",
    "                if target: break\n",
    "            if not target and labels: target = labels[-1]\n",
    "            if target:\n",
    "                old = first_cell_token(driver)\n",
    "                s.select_by_visible_text(target)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                tried = True\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Try toolbar buttons (10|25|50|100)\n",
    "    if not tried:\n",
    "        for want in (\"100\",\"200\"):\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[normalize-space()='{want}']\")\n",
    "                old = first_cell_token(driver)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ---------------- Full column extraction + filters ----------------\n",
    "def unique_headers(headers):\n",
    "    counts = defaultdict(int); out = []\n",
    "    for h in headers:\n",
    "        key = h or \"col\"; counts[key] += 1\n",
    "        out.append(key if counts[key] == 1 else f\"{key}_{counts[key]}\")\n",
    "    return out\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    ths = table.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "    headers = [th.get_attribute(\"innerText\").strip() or th.get_attribute(\"title\") or th.get_attribute(\"aria-label\") or \"\" for th in ths]\n",
    "    if not any(h for h in headers):\n",
    "        try:\n",
    "            first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "            tds = first_row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "            tmp = []\n",
    "            for td in tds:\n",
    "                label = td.get_attribute(\"data-title\") or td.get_attribute(\"aria-label\") or \"\"\n",
    "                tmp.append(label)\n",
    "            headers = tmp\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not headers:\n",
    "        first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "        n = len(first_row.find_elements(By.CSS_SELECTOR, \"td\"))\n",
    "        headers = [f\"col_{i+1}\" for i in range(n)]\n",
    "    return unique_headers([h.strip() for h in headers])\n",
    "\n",
    "def row_dicts_all_columns(driver, headers):\n",
    "    \"\"\"\n",
    "    Extract all visible columns and apply inclusion/exclusion filters.\n",
    "    Adds <column>_link if a link exists in that cell.\n",
    "    \"\"\"\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    rows = []\n",
    "    for tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "        tds = tr.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "        if not tds: continue\n",
    "        row = {}; concat = []\n",
    "        for i, td in enumerate(tds):\n",
    "            key = headers[i] if i < len(headers) else f\"col_{i+1}\"\n",
    "            text = td.get_attribute(\"innerText\").strip()\n",
    "            row[key] = text\n",
    "            concat.append(text)\n",
    "            try:\n",
    "                a = td.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                row[f\"{key}_link\"] = a.get_attribute(\"href\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        norm = _norm(\" | \".join(concat))\n",
    "        hits_incl = sorted({kw for kw in INCLUDE if kw in norm})\n",
    "        hits_excl = sorted({kw for kw in EXCLUDE if kw in norm})\n",
    "        if hits_incl and not hits_excl:\n",
    "            row[\"matched_keywords\"] = \", \".join(hits_incl)\n",
    "            # also store exclusions hit (for QA); empty in accepted rows\n",
    "            row[\"excluded_keywords_hit\"] = \", \".join(hits_excl)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def go_next(driver):\n",
    "    try:\n",
    "        nxt = driver.find_element(By.CSS_SELECTOR, \"a[rel='next']\")\n",
    "        if (nxt.get_attribute(\"aria-disabled\") or \"\").lower() == \"true\":\n",
    "            return False\n",
    "        token = first_cell_token(driver)\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != token)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15):\n",
    "    driver = make_driver(headless=headless)\n",
    "    driver.get(START_URL)\n",
    "    accept_cookies(driver)\n",
    "    wait_for_table(driver)\n",
    "    set_page_size_max(driver)\n",
    "\n",
    "    # optional: quick info\n",
    "    try:\n",
    "        info = driver.find_element(By.XPATH, \"//*[contains(., 'Zeige') and contains(., 'von')]\").text\n",
    "        m = re.search(r\"Zeige\\s+(\\d+)\\s+bis\\s+(\\d+)\\s+von\\s+(\\d+)\", info)\n",
    "        if m:\n",
    "            start, end, total = map(int, m.groups()); size = end - start + 1\n",
    "            pages = math.ceil(total / size) if size else None\n",
    "            if pages: print(f\"ℹ️ ~{pages} pages, page size {size}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    headers = get_table_headers(driver)\n",
    "    results, page = [], 1\n",
    "    while True:\n",
    "        print(f\"📄 Page {page} …\", end=\"\", flush=True)\n",
    "        page_rows = row_dicts_all_columns(driver, headers)\n",
    "        results.extend(page_rows)\n",
    "        print(f\" kept {len(page_rows)}\")\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        time.sleep(per_page_delay)\n",
    "        if not go_next(driver):\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save CSV\n",
    "    all_keys = set(headers + [f\"{h}_link\" for h in headers] + [\"matched_keywords\", \"excluded_keywords_hit\"])\n",
    "    for r in results: all_keys.update(r.keys())\n",
    "    preferred = [\"Bezeichnung\",\"Geschäftszeichen\",\"Vergabestelle\",\"Ort der Leistung\",\"matched_keywords\",\"excluded_keywords_hit\"]\n",
    "    header = [k for k in preferred if k in all_keys] + [k for k in sorted(all_keys) if k not in preferred]\n",
    "\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"evergabe_filtered_multilang_{ts}.csv\"\n",
    "    with open(out, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader(); w.writerows(results)\n",
    "    print(f\"✅ Saved {len(results)} rows to {out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # headless=False to watch it work. Set max_pages for a quick test run.\n",
    "    crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f701f",
   "metadata": {},
   "source": [
    "THIS IS IN GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aafa520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ ~127 pages with page size 10\n",
      "📄 Page 1 … kept 10\n",
      "📄 Page 2 … kept 10\n",
      "📄 Page 3 … kept 10\n",
      "📄 Page 4 … kept 9\n",
      "📄 Page 5 … kept 9\n",
      "📄 Page 6 … kept 10\n",
      "📄 Page 7 … kept 10\n",
      "📄 Page 8 … kept 10\n",
      "📄 Page 9 … kept 10\n",
      "📄 Page 10 … kept 10\n",
      "📄 Page 11 … kept 10\n",
      "📄 Page 12 … kept 10\n",
      "📄 Page 13 … kept 9\n",
      "📄 Page 14 … kept 10\n",
      "📄 Page 15 … kept 10\n",
      "📄 Page 16 … kept 10\n",
      "📄 Page 17 … kept 10\n",
      "📄 Page 18 … kept 10\n",
      "📄 Page 19 … kept 10\n",
      "📄 Page 20 … kept 10\n",
      "📄 Page 21 … kept 10\n",
      "📄 Page 22 … kept 8\n",
      "📄 Page 23 … kept 10\n",
      "📄 Page 24 … kept 10\n",
      "📄 Page 25 … kept 10\n",
      "📄 Page 26 … kept 10\n",
      "📄 Page 27 … kept 8\n",
      "📄 Page 28 … kept 8\n",
      "📄 Page 29 … kept 10\n",
      "📄 Page 30 … kept 10\n",
      "📄 Page 31 … kept 9\n",
      "📄 Page 32 … kept 9\n",
      "📄 Page 33 … kept 8\n",
      "📄 Page 34 … kept 9\n",
      "📄 Page 35 … kept 9\n",
      "📄 Page 36 … kept 9\n",
      "📄 Page 37 … kept 10\n",
      "📄 Page 38 … kept 10\n",
      "📄 Page 39 … kept 9\n",
      "📄 Page 40 … kept 10\n",
      "📄 Page 41 … kept 10\n",
      "📄 Page 42 … kept 10\n",
      "📄 Page 43 … kept 10\n",
      "📄 Page 44 … kept 9\n",
      "📄 Page 45 … kept 7\n",
      "📄 Page 46 … kept 9\n",
      "📄 Page 47 … kept 8\n",
      "📄 Page 48 … kept 10\n",
      "📄 Page 49 … kept 10\n",
      "📄 Page 50 … kept 10\n",
      "📄 Page 51 … kept 8\n",
      "📄 Page 52 … kept 9\n",
      "📄 Page 53 … kept 10\n",
      "📄 Page 54 … kept 10\n",
      "📄 Page 55 … kept 9\n",
      "📄 Page 56 … kept 10\n",
      "📄 Page 57 … kept 10\n",
      "📄 Page 58 … kept 10\n",
      "📄 Page 59 … kept 10\n",
      "📄 Page 60 … kept 10\n",
      "📄 Page 61 … kept 10\n",
      "📄 Page 62 … kept 10\n",
      "📄 Page 63 … kept 10\n",
      "📄 Page 64 … kept 10\n",
      "📄 Page 65 … kept 10\n",
      "📄 Page 66 … kept 10\n",
      "📄 Page 67 … kept 8\n",
      "📄 Page 68 … kept 10\n",
      "📄 Page 69 … kept 10\n",
      "📄 Page 70 … kept 10\n",
      "📄 Page 71 … kept 10\n",
      "📄 Page 72 … kept 10\n",
      "📄 Page 73 … kept 10\n",
      "📄 Page 74 … kept 8\n",
      "📄 Page 75 … kept 10\n",
      "📄 Page 76 … kept 10\n",
      "📄 Page 77 … kept 9\n",
      "📄 Page 78 … kept 9\n",
      "📄 Page 79 … kept 10\n",
      "📄 Page 80 … kept 10\n",
      "📄 Page 81 … kept 10\n",
      "📄 Page 82 … kept 7\n",
      "📄 Page 83 … kept 10\n",
      "📄 Page 84 … kept 10\n",
      "📄 Page 85 … kept 10\n",
      "📄 Page 86 … kept 9\n",
      "📄 Page 87 … kept 10\n",
      "📄 Page 88 … kept 10\n",
      "📄 Page 89 … kept 10\n",
      "📄 Page 90 … kept 10\n",
      "📄 Page 91 … kept 9\n",
      "📄 Page 92 … kept 10\n",
      "📄 Page 93 … kept 10\n",
      "📄 Page 94 … kept 8\n",
      "📄 Page 95 … kept 10\n",
      "📄 Page 96 … kept 10\n",
      "📄 Page 97 … kept 10\n",
      "📄 Page 98 … kept 8\n",
      "📄 Page 99 … kept 9\n",
      "📄 Page 100 … kept 10\n",
      "📄 Page 101 … kept 9\n",
      "📄 Page 102 … kept 9\n",
      "📄 Page 103 … kept 10\n",
      "📄 Page 104 … kept 8\n",
      "📄 Page 105 … kept 10\n",
      "📄 Page 106 … kept 9\n",
      "📄 Page 107 … kept 9\n",
      "📄 Page 108 … kept 10\n",
      "📄 Page 109 … kept 10\n",
      "📄 Page 110 … kept 6\n",
      "📄 Page 111 … kept 10\n",
      "📄 Page 112 … kept 10\n",
      "📄 Page 113 … kept 9\n",
      "📄 Page 114 … kept 8\n",
      "📄 Page 115 … kept 8\n",
      "📄 Page 116 … kept 10\n",
      "📄 Page 117 … kept 6\n",
      "📄 Page 118 … kept 9\n",
      "📄 Page 119 … kept 10\n",
      "📄 Page 120 … kept 9\n",
      "📄 Page 121 … kept 10\n",
      "📄 Page 122 … kept 7\n",
      "📄 Page 123 … kept 3\n",
      "📄 Page 124 … kept 3\n",
      "📄 Page 125 … kept 5\n",
      "📄 Page 126 … kept 6\n",
      "📄 Page 127 … kept 0\n",
      "✅ Saved 1169 rows to evergabe_keywords_fullcolumns_20251001_132540.csv\n"
     ]
    }
   ],
   "source": [
    "# FAST single-pass scraper with FULL columns\n",
    "# Keywords limited to user's list\n",
    "# pip install selenium webdriver-manager\n",
    "\n",
    "import csv, time, re, math, datetime, unicodedata\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "START_URL = \"https://www.evergabe-online.de/search.html?4\"\n",
    "\n",
    "# ---------------- KEYWORDS (ONLY THE ONES YOU GAVE) ----------------\n",
    "RAW_KW = [\n",
    "    # Study / Studie\n",
    "    \"Study\", \"Studie\",\n",
    "\n",
    "    # Analysis / Analyse\n",
    "    \"Analysis\", \"Analyse\",\n",
    "\n",
    "    # Economy / Wirtschaft / Ökonomie\n",
    "    \"Economy\", \"Wirtschaft\", \"Ökonomie\",\n",
    "\n",
    "    # Benchmarking\n",
    "    \"Benchmarking\",\n",
    "\n",
    "    # Wirtschaftsberatung\n",
    "    \"Wirtschaftsberatung\",\n",
    "\n",
    "    # Haupt-CPV … (use codes & main German phrases appearing with them)\n",
    "    \"72000000\", \"79300000\",\n",
    "    \"IT-Dienste\",  # aus \"IT-Dienste: Beratung, Software-Entwicklung, Internet und Hilfestellung\"\n",
    "    \"Beratung, Software-Entwicklung, Internet und Hilfestellung\",\n",
    "    \"Beratung, Markt- und Wirtschaftsforschung; Umfragen und Statistiken\",\n",
    "    \"Markt- und Wirtschaftsforschung; Umfragen und Statistiken\",\n",
    "\n",
    "    # Dienstleistung\n",
    "    \"Dienstleistung\",\n",
    "\n",
    "    # Ausschreibung\n",
    "    \"Ausschreibung\",\n",
    "\n",
    "    # Offenes Verfahren\n",
    "    \"Offenes Verfahren\",\n",
    "\n",
    "    # BKS / Bundesamt für Statistik, Office fédéral de la statistique\n",
    "    \"BKS\",\n",
    "    \"Bundesamt für Statistik\",\n",
    "    \"Office fédéral de la statistique\",\n",
    "\n",
    "    # SECO / Staatssekretariat für Wirtschaft\n",
    "    \"SECO\",\n",
    "    \"Staatssekretariat für Wirtschaft\",\n",
    "\n",
    "    # Kanton Zürich, Luzern etc…\n",
    "    \"Kanton Zürich\", \"Zürich\",\n",
    "    \"Kanton Luzern\", \"Luzern\",\n",
    "\n",
    "    # Bundesamt für Bauten und Logistik BBL\n",
    "    \"Bundesamt für Bauten und Logistik\",\n",
    "    \"BBL\",\n",
    "\n",
    "    # In general: Bundesamt für..\n",
    "    \"Bundesamt für\",\n",
    "\n",
    "    # Index (e.g. Verbraucherindex)\n",
    "    \"Index\", \"Verbraucherindex\",\n",
    "\n",
    "    # Regionen\n",
    "    \"Regionen\",\n",
    "\n",
    "    # Wirtschaftsforschung\n",
    "    \"Wirtschaftsforschung\",\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"\\s+\",\" \", s.strip().lower())\n",
    "\n",
    "KEYWORDS = {_norm(k) for k in RAW_KW if k.strip()}\n",
    "\n",
    "# ---------------- Selenium setup ----------------\n",
    "def make_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--lang=de-DE\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    for label in [\"Alle akzeptieren\",\"Akzeptieren\",\"Zustimmen\",\"Einverstanden\",\"OK\",\"Okay\",\"Ich stimme zu\"]:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//button[contains(., '{label}')]\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def wait_for_table(driver, timeout=20):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table tbody tr\"))\n",
    "    )\n",
    "\n",
    "def first_cell_token(driver):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"table tbody tr td\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def set_page_size_max(driver):\n",
    "    tried = False\n",
    "    for sel in [\"select\", \"select.page-size\", \"select[name*='size' i]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "            s = Select(el)\n",
    "            labels = [o.text.strip() or o.get_attribute(\"value\") for o in s.options]\n",
    "            target = None\n",
    "            for want in (\"100\",\"200\",\"Alle\",\"Alles\"):\n",
    "                for lbl in labels:\n",
    "                    if want in lbl:\n",
    "                        target = lbl; break\n",
    "                if target: break\n",
    "            if not target and labels: target = labels[-1]\n",
    "            if target:\n",
    "                old = first_cell_token(driver)\n",
    "                s.select_by_visible_text(target)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                tried = True\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not tried:\n",
    "        for want in (\"100\",\"200\"):\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[normalize-space()='{want}']\")\n",
    "                old = first_cell_token(driver)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# -------- FULL column extraction + per-row keyword match --------\n",
    "from collections import defaultdict\n",
    "def unique_headers(headers):\n",
    "    counts = defaultdict(int)\n",
    "    out = []\n",
    "    for h in headers:\n",
    "        key = h or \"col\"\n",
    "        counts[key] += 1\n",
    "        out.append(key if counts[key] == 1 else f\"{key}_{counts[key]}\")\n",
    "    return out\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    ths = table.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "    headers = [th.get_attribute(\"innerText\").strip() or th.get_attribute(\"title\") or th.get_attribute(\"aria-label\") or \"\" for th in ths]\n",
    "    if not any(h for h in headers):\n",
    "        try:\n",
    "            first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "            tds = first_row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "            tmp = []\n",
    "            for td in tds:\n",
    "                label = td.get_attribute(\"data-title\") or td.get_attribute(\"aria-label\") or \"\"\n",
    "                tmp.append(label)\n",
    "            headers = tmp\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not headers:\n",
    "        first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "        n = len(first_row.find_elements(By.CSS_SELECTOR, \"td\"))\n",
    "        headers = [f\"col_{i+1}\" for i in range(n)]\n",
    "    return unique_headers([h.strip() for h in headers])\n",
    "\n",
    "def row_dicts_all_columns(driver, headers):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    rows = []\n",
    "    for tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "        tds = tr.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "        if not tds:\n",
    "            continue\n",
    "        row = {}\n",
    "        concat = []\n",
    "        for i, td in enumerate(tds):\n",
    "            key = headers[i] if i < len(headers) else f\"col_{i+1}\"\n",
    "            text = td.get_attribute(\"innerText\").strip()\n",
    "            row[key] = text\n",
    "            concat.append(text)\n",
    "            try:\n",
    "                a = td.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                row[f\"{key}_link\"] = a.get_attribute(\"href\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        norm = _norm(\" | \".join(concat))\n",
    "        hits = [kw for kw in KEYWORDS if kw in norm]\n",
    "        if hits:\n",
    "            row[\"matched_keywords\"] = \", \".join(sorted(set(hits)))\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def go_next(driver):\n",
    "    try:\n",
    "        nxt = driver.find_element(By.CSS_SELECTOR, \"a[rel='next']\")\n",
    "        if (nxt.get_attribute(\"aria-disabled\") or \"\").lower() == \"true\":\n",
    "            return False\n",
    "        token = first_cell_token(driver)\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != token)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15):\n",
    "    driver = make_driver(headless=headless)\n",
    "    driver.get(START_URL)\n",
    "    accept_cookies(driver)\n",
    "    wait_for_table(driver)\n",
    "    set_page_size_max(driver)\n",
    "\n",
    "    # optional info\n",
    "    try:\n",
    "        info = driver.find_element(By.XPATH, \"//*[contains(., 'Zeige') and contains(., 'von')]\").text\n",
    "        m = re.search(r\"Zeige\\s+(\\d+)\\s+bis\\s+(\\d+)\\s+von\\s+(\\d+)\", info)\n",
    "        if m:\n",
    "            start, end, total = map(int, m.groups()); size = end - start + 1\n",
    "            pages = math.ceil(total / size) if size else None\n",
    "            if pages: print(f\"ℹ️ ~{pages} pages with page size {size}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    headers = get_table_headers(driver)\n",
    "    results, page = [], 1\n",
    "    while True:\n",
    "        print(f\"📄 Page {page} …\", end=\"\", flush=True)\n",
    "        page_rows = row_dicts_all_columns(driver, headers)\n",
    "        results.extend(page_rows)\n",
    "        print(f\" kept {len(page_rows)}\")\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        time.sleep(per_page_delay)\n",
    "        if not go_next(driver):\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # save CSV\n",
    "    all_keys = set(headers + [f\"{h}_link\" for h in headers] + [\"matched_keywords\"])\n",
    "    for r in results: all_keys.update(r.keys())\n",
    "    preferred = [\"Bezeichnung\",\"Geschäftszeichen\",\"Vergabestelle\",\"Ort der Leistung\"]\n",
    "    header = [k for k in preferred if k in all_keys] + [k for k in sorted(all_keys) if k not in preferred]\n",
    "\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"evergabe_keywords_fullcolumns_{ts}.csv\"\n",
    "    with open(out, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader(); w.writerows(results)\n",
    "    print(f\"✅ Saved {len(results)} rows to {out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # headless=False to watch; set max_pages for a quick test\n",
    "    crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257d2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Page 1 … kept 2\n",
      "📄 Page 2 … kept 5\n",
      "📄 Page 3 … kept 2\n",
      "📄 Page 4 … kept 2\n",
      "📄 Page 5 … kept 2\n",
      "📄 Page 6 … kept 1\n",
      "📄 Page 7 … kept 1\n",
      "📄 Page 8 … kept 1\n",
      "📄 Page 9 … kept 3\n",
      "📄 Page 10 … kept 2\n",
      "📄 Page 11 … kept 1\n",
      "📄 Page 12 … kept 2\n",
      "📄 Page 13 … kept 0\n",
      "📄 Page 14 … kept 2\n",
      "📄 Page 15 … kept 2\n",
      "📄 Page 16 … kept 2\n",
      "📄 Page 17 … kept 4\n",
      "📄 Page 18 … kept 4\n",
      "📄 Page 19 … kept 5\n",
      "📄 Page 20 … kept 2\n",
      "📄 Page 21 … kept 2\n",
      "📄 Page 22 … kept 4\n",
      "📄 Page 23 … kept 1\n",
      "📄 Page 24 … kept 3\n",
      "📄 Page 25 … kept 2\n",
      "📄 Page 26 … kept 3\n",
      "📄 Page 27 … kept 1\n",
      "📄 Page 28 … kept 1\n",
      "📄 Page 29 … kept 3\n",
      "📄 Page 30 … kept 1\n",
      "📄 Page 31 … kept 1\n",
      "📄 Page 32 … kept 0\n",
      "📄 Page 33 … kept 3\n",
      "📄 Page 34 … kept 4\n",
      "📄 Page 35 … kept 1\n",
      "📄 Page 36 … kept 2\n",
      "📄 Page 37 … kept 1\n",
      "📄 Page 38 … kept 1\n",
      "📄 Page 39 … kept 2\n",
      "📄 Page 40 … kept 2\n",
      "📄 Page 41 … kept 3\n",
      "📄 Page 42 … kept 3\n",
      "📄 Page 43 … kept 1\n",
      "📄 Page 44 … kept 2\n",
      "📄 Page 45 … kept 1\n",
      "📄 Page 46 … kept 3\n",
      "📄 Page 47 … kept 4\n",
      "📄 Page 48 … kept 0\n",
      "📄 Page 49 … kept 2\n",
      "📄 Page 50 … kept 0\n",
      "📄 Page 51 … kept 2\n",
      "📄 Page 52 … kept 0\n",
      "📄 Page 53 … kept 4\n",
      "📄 Page 54 … kept 2\n",
      "📄 Page 55 … kept 1\n",
      "📄 Page 56 … kept 1\n",
      "📄 Page 57 … kept 2\n",
      "📄 Page 58 … kept 1\n",
      "📄 Page 59 … kept 4\n",
      "📄 Page 60 … kept 3\n",
      "📄 Page 61 … kept 1\n",
      "📄 Page 62 … kept 3\n",
      "📄 Page 63 … kept 1\n",
      "📄 Page 64 … kept 1\n",
      "📄 Page 65 … kept 3\n",
      "📄 Page 66 … kept 2\n",
      "📄 Page 67 … kept 3\n",
      "📄 Page 68 … kept 3\n",
      "📄 Page 69 … kept 4\n",
      "📄 Page 70 … kept 2\n",
      "📄 Page 71 … kept 3\n",
      "📄 Page 72 … kept 1\n",
      "📄 Page 73 … kept 1\n",
      "📄 Page 74 … kept 1\n",
      "📄 Page 75 … kept 2\n",
      "📄 Page 76 … kept 1\n",
      "📄 Page 77 … kept 0\n",
      "📄 Page 78 … kept 3\n",
      "📄 Page 79 … kept 0\n",
      "📄 Page 80 … kept 3\n",
      "📄 Page 81 … kept 1\n",
      "📄 Page 82 … kept 3\n",
      "📄 Page 83 … kept 1\n",
      "📄 Page 84 … kept 2\n",
      "📄 Page 85 … kept 3\n",
      "📄 Page 86 … kept 3\n",
      "📄 Page 87 … kept 0\n",
      "📄 Page 88 … kept 2\n",
      "📄 Page 89 … kept 4\n",
      "📄 Page 90 … kept 5\n",
      "📄 Page 91 … kept 0\n",
      "📄 Page 92 … kept 4\n",
      "📄 Page 93 … kept 1\n",
      "📄 Page 94 … kept 2\n",
      "📄 Page 95 … kept 1\n",
      "📄 Page 96 … kept 0\n",
      "📄 Page 97 … kept 3\n",
      "📄 Page 98 … kept 5\n",
      "📄 Page 99 … kept 2\n",
      "📄 Page 100 … kept 3\n",
      "📄 Page 101 … kept 4\n",
      "📄 Page 102 … kept 5\n",
      "📄 Page 103 … kept 0\n",
      "📄 Page 104 … kept 0\n",
      "📄 Page 105 … kept 3\n",
      "📄 Page 106 … kept 1\n",
      "📄 Page 107 … kept 2\n",
      "📄 Page 108 … kept 4\n",
      "📄 Page 109 … kept 2\n",
      "📄 Page 110 … kept 0\n",
      "📄 Page 111 … kept 1\n",
      "📄 Page 112 … kept 3\n",
      "📄 Page 113 … kept 4\n",
      "📄 Page 114 … kept 2\n",
      "📄 Page 115 … kept 2\n",
      "📄 Page 116 … kept 1\n",
      "📄 Page 117 … kept 4\n",
      "📄 Page 118 … kept 2\n",
      "📄 Page 119 … kept 0\n",
      "📄 Page 120 … kept 2\n",
      "📄 Page 121 … kept 8\n",
      "📄 Page 122 … kept 0\n",
      "📄 Page 123 … kept 0\n",
      "📄 Page 124 … kept 3\n",
      "📄 Page 125 … kept 1\n",
      "📄 Page 126 … kept 3\n",
      "📄 Page 127 … kept 2\n",
      "📄 Page 128 … kept 1\n",
      "✅ Saved 265 rows to evergabe_filtered_with_details_20251001_153338.csv\n",
      "📂 PDFs (if found) are in: c:\\Users\\damla\\Desktop\\downloads\n"
     ]
    }
   ],
   "source": [
    "# evergabe_singlepass_incl_excl_multilang_with_details.py\n",
    "# FAST single-pass crawl with:\n",
    "# - multilingual inclusion/exclusion filters\n",
    "# - full column extraction (+ per-cell links)\n",
    "# - stable unique id per row\n",
    "# - visit each detail_url: grab text content and download first PDF\n",
    "# - CSV output + a /downloads folder with PDFs\n",
    "\n",
    "import csv, time, re, math, datetime, unicodedata, os, uuid, pathlib\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "START_URL = \"https://www.evergabe-online.de/search.html?4\"\n",
    "DOWNLOAD_DIR = \"downloads\"  # PDFs will be saved here\n",
    "\n",
    "# ---------------- INCLUSION KEYWORDS (DE/FR/IT/EN) ----------------\n",
    "RAW_INCLUDE = [\n",
    "    \"study\", \"studie\", \"étude\", \"etude\", \"studio\",\n",
    "    \"analysis\", \"analyse\", \"analisi\",\n",
    "    \"economy\", \"economics\", \"wirtschaft\", \"ökonomie\", \"economie\", \"economia\",\n",
    "    \"benchmark\", \"benchmarking\",\n",
    "    \"wirtschaftsberatung\", \"consulting\", \"conseil\", \"consulenza\",\n",
    "    \"72000000\", \"79300000\",\n",
    "    \"it-dienste\",\n",
    "    \"beratung, software-entwicklung, internet und hilfestellung\",\n",
    "    \"markt- und wirtschaftsforschung\", \"umfragen und statistiken\",\n",
    "    \"dienstleistung\", \"service\", \"prestation\", \"prestazione\",\n",
    "    \"ausschreibung\", \"appel d'offres\", \"appels d'offres\", \"gara\", \"procurement\",\n",
    "    \"offenes verfahren\", \"procédure ouverte\", \"procedure ouverte\", \"procedura aperta\",\n",
    "    \"bks\", \"bundesamt für statistik\",\n",
    "    \"office fédéral de la statistique\", \"office federal de la statistique\",\n",
    "    \"ufficio federale di statistica\", \"federal statistical office\", \"ofs\", \"bfs\",\n",
    "    \"seco\", \"staatssekretariat für wirtschaft\",\n",
    "    \"secrétariat d'état à l'économie\", \"secretariat d'etat a l'economie\",\n",
    "    \"segretariato di stato dell'economia\",\n",
    "    \"kanton zürich\", \"kanton zuerich\", \"canton de zurich\", \"cantone zurigo\",\n",
    "    \"kanton luzern\", \"canton de lucerne\", \"cantone lucerna\",\n",
    "    \"kanton\", \"canton\", \"cantone\",\n",
    "    \"region\", \"regionen\", \"région\", \"regione\",\n",
    "    \"bundesamt für\", \"bundesamt fuer\",\n",
    "    \"bundesamt für bauten und logistik\", \"bundesamt fuer bauten und logistik\", \"bbl\",\n",
    "    \"index\", \"verbraucherindex\", \"price index\", \"indice des prix\", \"indice dei prezzi\",\n",
    "    \"wirtschaftsforschung\", \"economic research\", \"recherche économique\",\n",
    "    \"recherche economique\", \"ricerca economica\",\n",
    "]\n",
    "\n",
    "# ---------------- EXCLUSION KEYWORDS (DE/FR/IT/EN) ----------------\n",
    "RAW_EXCLUDE = [\n",
    "    \"construction\", \"bau\", \"bâtiment\", \"batiment\", \"costruzioni\",\n",
    "    \"health care\", \"gesundheit\", \"santé\", \"sante\", \"sanità\", \"sanita\",\n",
    "    \"transport\", \"verkehr\", \"transports\", \"trasporti\",\n",
    "    \"mobility\", \"mobilität\", \"mobilite\", \"mobilité\", \"mobilita\",\n",
    "    \"sport\",\n",
    "    \"culture\", \"kultur\", \"cultura\",\n",
    "    \"street\", \"strasse\", \"straße\", \"rue\", \"strada\",\n",
    "    \"infrastructure\", \"infrastruktur\", \"infrastruttura\",\n",
    "    \"process\", \"prozess\", \"processus\", \"processo\",\n",
    "    \"it\", \"informatik\", \"informatique\", \"informatica\",\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "INCLUDE = {_norm(k) for k in RAW_INCLUDE if k.strip()}\n",
    "EXCLUDE = {_norm(k) for k in RAW_EXCLUDE if k.strip()}\n",
    "\n",
    "# ---------------- Selenium setup ----------------\n",
    "def make_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--lang=de-DE\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    labels = [\"Alle akzeptieren\",\"Akzeptieren\",\"Zustimmen\",\"Einverstanden\",\"OK\",\"Okay\",\"Ich stimme zu\"]\n",
    "    for label in labels:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//button[contains(., '{label}')]\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def wait_for_table(driver, timeout=20):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table tbody tr\"))\n",
    "    )\n",
    "\n",
    "def first_cell_token(driver):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"table tbody tr td\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def set_page_size_max(driver):\n",
    "    tried = False\n",
    "    for sel in [\"select\", \"select.page-size\", \"select[name*='size' i]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "            s = Select(el)\n",
    "            labels = [o.text.strip() or o.get_attribute(\"value\") for o in s.options]\n",
    "            target = None\n",
    "            for want in (\"100\",\"200\",\"Alle\",\"Alles\"):\n",
    "                for lbl in labels:\n",
    "                    if want in lbl:\n",
    "                        target = lbl; break\n",
    "                if target: break\n",
    "            if not target and labels: target = labels[-1]\n",
    "            if target:\n",
    "                old = first_cell_token(driver)\n",
    "                s.select_by_visible_text(target)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                tried = True\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not tried:\n",
    "        for want in (\"100\",\"200\"):\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[normalize-space()='{want}']\")\n",
    "                old = first_cell_token(driver)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ---------------- Full column extraction + filters ----------------\n",
    "def unique_headers(headers):\n",
    "    counts = defaultdict(int); out = []\n",
    "    for h in headers:\n",
    "        key = h or \"col\"; counts[key] += 1\n",
    "        out.append(key if counts[key] == 1 else f\"{key}_{counts[key]}\")\n",
    "    return out\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    ths = table.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "    headers = [th.get_attribute(\"innerText\").strip() or th.get_attribute(\"title\") or th.get_attribute(\"aria-label\") or \"\" for th in ths]\n",
    "    if not any(h for h in headers):\n",
    "        try:\n",
    "            first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "            tds = first_row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "            tmp = []\n",
    "            for td in tds:\n",
    "                label = td.get_attribute(\"data-title\") or td.get_attribute(\"aria-label\") or \"\"\n",
    "                tmp.append(label)\n",
    "            headers = tmp\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not headers:\n",
    "        first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "        n = len(first_row.find_elements(By.CSS_SELECTOR, \"td\"))\n",
    "        headers = [f\"col_{i+1}\" for i in range(n)]\n",
    "    return unique_headers([h.strip() for h in headers])\n",
    "\n",
    "def row_dicts_all_columns(driver, headers):\n",
    "    \"\"\"\n",
    "    Extract all visible columns and apply inclusion/exclusion filters.\n",
    "    Adds <column>_link if a link exists in that cell.\n",
    "    \"\"\"\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    rows = []\n",
    "    for tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "        tds = tr.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "        if not tds: continue\n",
    "        row = {}; concat = []\n",
    "        for i, td in enumerate(tds):\n",
    "            key = headers[i] if i < len(headers) else f\"col_{i+1}\"\n",
    "            text = td.get_attribute(\"innerText\").strip()\n",
    "            row[key] = text\n",
    "            concat.append(text)\n",
    "            try:\n",
    "                a = td.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                row[f\"{key}_link\"] = a.get_attribute(\"href\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        norm = _norm(\" | \".join(concat))\n",
    "        hits_incl = sorted({kw for kw in INCLUDE if kw in norm})\n",
    "        hits_excl = sorted({kw for kw in EXCLUDE if kw in norm})\n",
    "        if hits_incl and not hits_excl:\n",
    "            row[\"matched_keywords\"] = \", \".join(hits_incl)\n",
    "            row[\"excluded_keywords_hit\"] = \", \".join(hits_excl)  # should be empty on kept rows\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def go_next(driver):\n",
    "    try:\n",
    "        nxt = driver.find_element(By.CSS_SELECTOR, \"a[rel='next']\")\n",
    "        if (nxt.get_attribute(\"aria-disabled\") or \"\").lower() == \"true\":\n",
    "            return False\n",
    "        token = first_cell_token(driver)\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != token)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ---------------- Detail page scraping & PDF download ----------------\n",
    "def selenium_cookies_to_requests(driver) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    for c in driver.get_cookies():\n",
    "        s.cookies.set(c[\"name\"], c[\"value\"], domain=c.get(\"domain\"))\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; EvergabeScraper/1.0)\"\n",
    "    })\n",
    "    return s\n",
    "\n",
    "def ensure_dir(path):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_detail_content_and_pdf(driver, url, req_session: requests.Session, timeout=25):\n",
    "    \"\"\"\n",
    "    Open the detail URL with Selenium, scrape visible text content (main area),\n",
    "    find the first PDF link, download it via requests (using Selenium cookies).\n",
    "    Returns (detail_text, pdf_local_path or \"\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # wait for something meaningful to be present\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "        )\n",
    "        # try to locate a main content area; fallback to all text\n",
    "        selectors = [\n",
    "            \"main\", \"article\", \"#content\", \".content\", \".main\", \".container\",\n",
    "            \".detail\", \".panel-body\"\n",
    "        ]\n",
    "        text = \"\"\n",
    "        for sel in selectors:\n",
    "            els = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            if els:\n",
    "                text = els[0].get_attribute(\"innerText\").strip()\n",
    "                if len(text) > 100:  # looks like real content\n",
    "                    break\n",
    "        if not text:\n",
    "            text = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # find first PDF link\n",
    "    pdf_url = \"\"\n",
    "    try:\n",
    "        a_tags = driver.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "        for a in a_tags:\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            if \".pdf\" in href.lower():\n",
    "                pdf_url = href\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    pdf_path = \"\"\n",
    "    if pdf_url:\n",
    "        try:\n",
    "            ensure_dir(DOWNLOAD_DIR)\n",
    "            # make a deterministic filename from URL\n",
    "            pdf_uid = uuid.uuid5(uuid.NAMESPACE_URL, pdf_url)\n",
    "            dest = os.path.join(DOWNLOAD_DIR, f\"{pdf_uid}.pdf\")\n",
    "            if not os.path.exists(dest):\n",
    "                r = req_session.get(pdf_url, timeout=timeout)\n",
    "                if r.ok and r.content[:4] in (b\"%PDF\",):  # naive PDF signature check\n",
    "                    with open(dest, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    pdf_path = dest\n",
    "            else:\n",
    "                pdf_path = dest\n",
    "        except Exception:\n",
    "            pdf_path = \"\"\n",
    "\n",
    "    return text, pdf_path\n",
    "\n",
    "# ---------------- Main crawl ----------------\n",
    "def crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15):\n",
    "    driver = make_driver(headless=headless)\n",
    "    driver.get(START_URL)\n",
    "    accept_cookies(driver)\n",
    "    wait_for_table(driver)\n",
    "    set_page_size_max(driver)\n",
    "\n",
    "    headers = get_table_headers(driver)\n",
    "    results, page = [], 1\n",
    "    while True:\n",
    "        print(f\"📄 Page {page} …\", end=\"\", flush=True)\n",
    "        page_rows = row_dicts_all_columns(driver, headers)\n",
    "        results.extend(page_rows)\n",
    "        print(f\" kept {len(page_rows)}\")\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        time.sleep(per_page_delay)\n",
    "        if not go_next(driver):\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    # Build unique ids now (stable by URL+Bezeichnung fallback)\n",
    "    for idx, r in enumerate(results, 1):\n",
    "        base = r.get(\"detail_url\") or r.get(\"Bezeichnung\") or str(idx)\n",
    "        # stable UUID v5 (same input -> same uid)\n",
    "        r[\"uid\"] = \"EVG-\" + uuid.uuid5(uuid.NAMESPACE_URL, base).hex[:12].upper()\n",
    "\n",
    "    # Use requests with Selenium cookies for downloads\n",
    "    req_sess = selenium_cookies_to_requests(driver)\n",
    "\n",
    "    # Visit each detail_url, collect content + pdf (first found)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        url = r.get(\"detail_url\", \"\")\n",
    "        if not url:\n",
    "            r[\"detail_content\"] = \"\"\n",
    "            r[\"pdf_path\"] = \"\"\n",
    "            continue\n",
    "        print(f\"   ↪ details [{i}/{len(results)}]: {url}\")\n",
    "        text, pdf_local = extract_detail_content_and_pdf(driver, url, req_sess)\n",
    "        r[\"detail_content\"] = text\n",
    "        r[\"pdf_path\"] = pdf_local\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save CSV\n",
    "    all_keys = set(headers + [f\"{h}_link\" for h in headers] + [\"matched_keywords\",\"excluded_keywords_hit\",\"uid\",\"detail_content\",\"pdf_path\"])\n",
    "    for r in results: all_keys.update(r.keys())\n",
    "    preferred = [\"uid\",\"Bezeichnung\",\"Geschäftszeichen\",\"Vergabestelle\",\"Ort der Leistung\",\"matched_keywords\",\"detail_url\",\"pdf_path\",\"detail_content\"]\n",
    "    header = [k for k in preferred if k in all_keys] + [k for k in sorted(all_keys) if k not in preferred]\n",
    "\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"evergabe_filtered_with_details_{ts}.csv\"\n",
    "    with open(out, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader(); w.writerows(results)\n",
    "\n",
    "    print(f\"✅ Saved {len(results)} rows to {out}\")\n",
    "    print(f\"📂 PDFs (if found) are in: {os.path.abspath(DOWNLOAD_DIR)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # headless=False to watch; set max_pages for a quick test\n",
    "    crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Page 1 … kept 3\n",
      "📄 Page 2 … kept 4\n",
      "📄 Page 3 … kept 2\n",
      "📄 Page 4 … kept 3\n",
      "📄 Page 5 … kept 1\n",
      "📄 Page 6 … kept 1\n",
      "📄 Page 7 … kept 2\n",
      "📄 Page 8 … kept 1\n",
      "📄 Page 9 … kept 3\n",
      "📄 Page 10 … kept 1\n",
      "📄 Page 11 … kept 1\n",
      "📄 Page 12 … kept 2\n",
      "📄 Page 13 … kept 0\n",
      "📄 Page 14 … kept 3\n",
      "📄 Page 15 … kept 1\n",
      "📄 Page 16 … kept 2\n",
      "📄 Page 17 … kept 5\n",
      "📄 Page 18 … kept 5\n",
      "📄 Page 19 … kept 4\n",
      "📄 Page 20 … kept 1\n",
      "📄 Page 21 … kept 2\n",
      "📄 Page 22 … kept 4\n",
      "📄 Page 23 … kept 1\n",
      "📄 Page 24 … kept 3\n",
      "📄 Page 25 … kept 2\n",
      "📄 Page 26 … kept 3\n",
      "📄 Page 27 … kept 2\n",
      "📄 Page 28 … kept 1\n",
      "📄 Page 29 … kept 2\n",
      "📄 Page 30 … kept 2\n",
      "📄 Page 31 … kept 0\n",
      "📄 Page 32 … kept 1\n",
      "📄 Page 33 … kept 3\n",
      "📄 Page 34 … kept 4\n",
      "📄 Page 35 … kept 0\n",
      "📄 Page 36 … kept 2\n",
      "📄 Page 37 … kept 1\n",
      "📄 Page 38 … kept 2\n",
      "📄 Page 39 … kept 2\n",
      "📄 Page 40 … kept 1\n",
      "📄 Page 41 … kept 4\n",
      "📄 Page 42 … kept 2\n",
      "📄 Page 43 … kept 1\n",
      "📄 Page 44 … kept 2\n",
      "📄 Page 45 … kept 2\n",
      "📄 Page 46 … kept 3\n",
      "📄 Page 47 … kept 3\n",
      "📄 Page 48 … kept 0\n",
      "📄 Page 49 … kept 2\n",
      "📄 Page 50 … kept 0\n",
      "📄 Page 51 … kept 2\n",
      "📄 Page 52 … kept 1\n",
      "📄 Page 53 … kept 4\n",
      "📄 Page 54 … kept 1\n",
      "📄 Page 55 … kept 1\n",
      "📄 Page 56 … kept 2\n",
      "📄 Page 57 … kept 2\n",
      "📄 Page 58 … kept 0\n",
      "📄 Page 59 … kept 4\n",
      "📄 Page 60 … kept 4\n",
      "📄 Page 61 … kept 0\n",
      "📄 Page 62 … kept 3\n",
      "📄 Page 63 … kept 1\n",
      "📄 Page 64 … kept 2\n",
      "📄 Page 65 … kept 2\n",
      "📄 Page 66 … kept 4\n",
      "📄 Page 67 … kept 1\n",
      "📄 Page 68 … kept 3\n",
      "📄 Page 69 … kept 5\n",
      "📄 Page 70 … kept 2\n",
      "📄 Page 71 … kept 2\n",
      "📄 Page 72 … kept 1\n",
      "📄 Page 73 … kept 1\n",
      "📄 Page 74 … kept 1\n",
      "📄 Page 75 … kept 2\n",
      "📄 Page 76 … kept 1\n",
      "📄 Page 77 … kept 1\n",
      "📄 Page 78 … kept 2\n",
      "📄 Page 79 … kept 1\n",
      "📄 Page 80 … kept 2\n",
      "📄 Page 81 … kept 2\n",
      "📄 Page 82 … kept 2\n",
      "📄 Page 83 … kept 1\n",
      "📄 Page 84 … kept 2\n",
      "📄 Page 85 … kept 4\n",
      "📄 Page 86 … kept 2\n",
      "📄 Page 87 … kept 0\n",
      "📄 Page 88 … kept 2\n",
      "📄 Page 89 … kept 5\n",
      "📄 Page 90 … kept 4\n",
      "📄 Page 91 … kept 1\n",
      "📄 Page 92 … kept 3\n",
      "📄 Page 93 … kept 1\n",
      "📄 Page 94 … kept 2\n",
      "📄 Page 95 … kept 1\n",
      "📄 Page 96 … kept 1\n",
      "📄 Page 97 … kept 2\n",
      "📄 Page 98 … kept 5\n",
      "📄 Page 99 … kept 2\n",
      "📄 Page 100 … kept 4\n",
      "📄 Page 101 … kept 4\n",
      "📄 Page 102 … kept 4\n",
      "📄 Page 103 … kept 0\n",
      "📄 Page 104 … kept 1\n",
      "📄 Page 105 … kept 2\n",
      "📄 Page 106 … kept 2\n",
      "📄 Page 107 … kept 1\n",
      "📄 Page 108 … kept 4\n",
      "📄 Page 109 … kept 2\n",
      "📄 Page 110 … kept 0\n",
      "📄 Page 111 … kept 2\n",
      "📄 Page 112 … kept 2\n",
      "📄 Page 113 … kept 4\n",
      "📄 Page 114 … kept 2\n",
      "📄 Page 115 … kept 2\n",
      "📄 Page 116 … kept 2\n",
      "📄 Page 117 … kept 4\n",
      "📄 Page 118 … kept 1\n",
      "📄 Page 119 … kept 0\n",
      "📄 Page 120 … kept 3\n",
      "📄 Page 121 … kept 7\n",
      "📄 Page 122 … kept 0\n",
      "📄 Page 123 … kept 1\n",
      "📄 Page 124 … kept 2\n",
      "📄 Page 125 … kept 1\n",
      "📄 Page 126 … kept 3\n",
      "📄 Page 127 … kept 2\n",
      "📄 Page 128 … kept 1\n",
      "✅ Saved 265 rows to evergabe_filtered_with_pdfs_20251001_151348.csv\n",
      "📂 PDFs saved under: c:\\Users\\damla\\Desktop\\downloads\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c01eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evergabe_singlepass_with_details_and_pdfs.py\n",
    "# FAST single-pass crawler with:\n",
    "# - multilingual inclusion/exclusion filters\n",
    "# - full column extraction (+ per-cell links)\n",
    "# - per-row UID\n",
    "# - detail page scraping (detail_content)\n",
    "# - download ALL PDFs per row -> downloads/<uid>_N.pdf\n",
    "\n",
    "import csv, time, re, math, datetime, unicodedata, os, uuid, pathlib\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "START_URL = \"https://www.evergabe-online.de/search.html?4\"\n",
    "DOWNLOAD_DIR = \"downloads\"  # PDFs saved here\n",
    "\n",
    "# ---------------- Inclusion / Exclusion keywords (DE/FR/IT/EN) ----------------\n",
    "RAW_INCLUDE = [\n",
    "    \"study\",\"studie\",\"étude\",\"etude\",\"studio\",\n",
    "    \"analysis\",\"analyse\",\"analisi\",\n",
    "    \"economy\",\"economics\",\"wirtschaft\",\"ökonomie\",\"economie\",\"economia\",\n",
    "    \"benchmark\",\"benchmarking\",\n",
    "    \"wirtschaftsberatung\",\"consulting\",\"conseil\",\"consulenza\",\n",
    "    \"72000000\",\"79300000\",\n",
    "    \"it-dienste\",\n",
    "    \"beratung, software-entwicklung, internet und hilfestellung\",\n",
    "    \"markt- und wirtschaftsforschung\",\"umfragen und statistiken\",\n",
    "    \"dienstleistung\",\"service\",\"prestation\",\"prestazione\",\n",
    "    \"ausschreibung\",\"appel d'offres\",\"appels d'offres\",\"gara\",\"procurement\",\n",
    "    \"offenes verfahren\",\"procédure ouverte\",\"procedure ouverte\",\"procedura aperta\",\n",
    "    \"bks\",\"bundesamt für statistik\",\"office fédéral de la statistique\",\"office federal de la statistique\",\n",
    "    \"ufficio federale di statistica\",\"federal statistical office\",\"ofs\",\"bfs\",\n",
    "    \"seco\",\"staatssekretariat für wirtschaft\",\n",
    "    \"secrétariat d'état à l'économie\",\"secretariat d'etat a l'economie\",\n",
    "    \"segretariato di stato dell'economia\",\n",
    "    \"kanton zürich\",\"kanton zuerich\",\"canton de zurich\",\"cantone zurigo\",\n",
    "    \"kanton luzern\",\"canton de lucerne\",\"cantone lucerna\",\n",
    "    \"kanton\",\"canton\",\"cantone\",\n",
    "    \"region\",\"regionen\",\"région\",\"regione\",\n",
    "    \"bundesamt für\",\"bundesamt fuer\",\n",
    "    \"bundesamt für bauten und logistik\",\"bundesamt fuer bauten und logistik\",\"bbl\",\n",
    "    \"index\",\"verbraucherindex\",\"price index\",\"indice des prix\",\"indice dei prezzi\",\n",
    "    \"wirtschaftsforschung\",\"economic research\",\"recherche économique\",\"recherche economique\",\"ricerca economica\",\n",
    "]\n",
    "RAW_EXCLUDE = [\n",
    "    \"construction\",\"bau\",\"bâtiment\",\"batiment\",\"costruzioni\",\n",
    "    \"health care\",\"gesundheit\",\"santé\",\"sante\",\"sanità\",\"sanita\",\n",
    "    \"transport\",\"verkehr\",\"transports\",\"trasporti\",\n",
    "    \"mobility\",\"mobilität\",\"mobilite\",\"mobilité\",\"mobilita\",\n",
    "    \"sport\",\n",
    "    \"culture\",\"kultur\",\"cultura\",\n",
    "    \"street\",\"strasse\",\"straße\",\"rue\",\"strada\",\n",
    "    \"infrastructure\",\"infrastruktur\",\"infrastruttura\",\n",
    "    \"process\",\"prozess\",\"processus\",\"processo\",\n",
    "    \"it\",\"informatik\",\"informatique\",\"informatica\",\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "INCLUDE = {_norm(k) for k in RAW_INCLUDE if k.strip()}\n",
    "EXCLUDE = {_norm(k) for k in RAW_EXCLUDE if k.strip()}\n",
    "\n",
    "# ---------------- Selenium setup ----------------\n",
    "def make_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--lang=de-DE\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    labels = [\"Alle akzeptieren\",\"Akzeptieren\",\"Zustimmen\",\"Einverstanden\",\"OK\",\"Okay\",\"Ich stimme zu\"]\n",
    "    for label in labels:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//button[contains(., '{label}')]\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def wait_for_table(driver, timeout=20):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table tbody tr\"))\n",
    "    )\n",
    "\n",
    "def first_cell_token(driver):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"table tbody tr td\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def set_page_size_max(driver):\n",
    "    tried = False\n",
    "    # Try <select>\n",
    "    for sel in [\"select\", \"select.page-size\", \"select[name*='size' i]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "            s = Select(el)\n",
    "            labels = [o.text.strip() or o.get_attribute(\"value\") for o in s.options]\n",
    "            target = None\n",
    "            for want in (\"100\",\"200\",\"Alle\",\"Alles\"):\n",
    "                for lbl in labels:\n",
    "                    if want in lbl:\n",
    "                        target = lbl; break\n",
    "                if target: break\n",
    "            if not target and labels: target = labels[-1]\n",
    "            if target:\n",
    "                old = first_cell_token(driver)\n",
    "                s.select_by_visible_text(target)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                tried = True\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Try buttons\n",
    "    if not tried:\n",
    "        for want in (\"100\",\"200\"):\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[normalize-space()='{want}']\")\n",
    "                old = first_cell_token(driver)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ---------------- Full column extraction + filters ----------------\n",
    "def unique_headers(headers):\n",
    "    counts = defaultdict(int); out = []\n",
    "    for h in headers:\n",
    "        key = h or \"col\"; counts[key] += 1\n",
    "        out.append(key if counts[key] == 1 else f\"{key}_{counts[key]}\")\n",
    "    return out\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    ths = table.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "    headers = [th.get_attribute(\"innerText\").strip() or th.get_attribute(\"title\") or th.get_attribute(\"aria-label\") or \"\" for th in ths]\n",
    "    if not any(h for h in headers):\n",
    "        try:\n",
    "            first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "            tds = first_row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "            tmp = []\n",
    "            for td in tds:\n",
    "                label = td.get_attribute(\"data-title\") or td.get_attribute(\"aria-label\") or \"\"\n",
    "                tmp.append(label)\n",
    "            headers = tmp\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not headers:\n",
    "        first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "        n = len(first_row.find_elements(By.CSS_SELECTOR, \"td\"))\n",
    "        headers = [f\"col_{i+1}\" for i in range(n)]\n",
    "    return unique_headers([h.strip() for h in headers])\n",
    "\n",
    "def row_dicts_all_columns(driver, headers):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    rows = []\n",
    "    for tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "        tds = tr.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "        if not tds: continue\n",
    "        row = {}; concat = []\n",
    "        for i, td in enumerate(tds):\n",
    "            key = headers[i] if i < len(headers) else f\"col_{i+1}\"\n",
    "            text = td.get_attribute(\"innerText\").strip()\n",
    "            row[key] = text\n",
    "            concat.append(text)\n",
    "            try:\n",
    "                a = td.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                row[f\"{key}_link\"] = a.get_attribute(\"href\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        norm = _norm(\" | \".join(concat))\n",
    "        hits_incl = sorted({kw for kw in INCLUDE if kw in norm})\n",
    "        hits_excl = sorted({kw for kw in EXCLUDE if kw in norm})\n",
    "        if hits_incl and not hits_excl:\n",
    "            row[\"matched_keywords\"] = \", \".join(hits_incl)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def go_next(driver):\n",
    "    try:\n",
    "        nxt = driver.find_element(By.CSS_SELECTOR, \"a[rel='next']\")\n",
    "        if (nxt.get_attribute(\"aria-disabled\") or \"\").lower() == \"true\":\n",
    "            return False\n",
    "        token = first_cell_token(driver)\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != token)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ---------------- Detail page scraping & PDFs ----------------\n",
    "def ensure_dir(path):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def selenium_cookies_to_requests(driver) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    for c in driver.get_cookies():\n",
    "        s.cookies.set(c[\"name\"], c[\"value\"], domain=c.get(\"domain\"))\n",
    "    s.headers.update({\"User-Agent\": \"Mozilla/5.0 (compatible; EvergabeScraper/1.0)\"})\n",
    "    return s\n",
    "\n",
    "def collect_pdf_links(driver, base_url):\n",
    "    \"\"\"Find ALL candidate PDF links on the current page.\"\"\"\n",
    "    found = set()\n",
    "    # direct anchors with .pdf\n",
    "    for a in driver.find_elements(By.CSS_SELECTOR, \"a[href]\"):\n",
    "        href = a.get_attribute(\"href\") or \"\"\n",
    "        text = (a.text or \"\").lower()\n",
    "        typ = (a.get_attribute(\"type\") or \"\").lower()\n",
    "        if \".pdf\" in href.lower() or \"pdf\" in text or \"application/pdf\" in typ:\n",
    "            found.add(urljoin(base_url, href))\n",
    "    # also look in <embed> or <object>\n",
    "    for e in driver.find_elements(By.CSS_SELECTOR, \"embed[src], object[data]\"):\n",
    "        src = e.get_attribute(\"src\") or e.get_attribute(\"data\") or \"\"\n",
    "        if \".pdf\" in src.lower():\n",
    "            found.add(urljoin(base_url, src))\n",
    "    return sorted(found)\n",
    "\n",
    "def click_possible_docs_tabs(driver):\n",
    "    \"\"\"Try to reveal documents area (common tab labels).\"\"\"\n",
    "    labels = [\"Dokumente\",\"Vergabeunterlagen\",\"Unterlagen\",\"Documents\",\"Documenti\",\"Documents\"]\n",
    "    for lab in labels:\n",
    "        try:\n",
    "            el = driver.find_element(By.XPATH, f\"//a[contains(., '{lab}') or //button[contains(., '{lab}')]]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", el)\n",
    "            time.sleep(0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def fetch_detail_and_pdfs(driver, detail_url, uid, req_sess, max_pdfs=10, timeout=30):\n",
    "    \"\"\"Open detail page, extract text and download ALL PDFs. Returns (text, [paths]).\"\"\"\n",
    "    driver.get(detail_url)\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "    # try to open doc tabs if any\n",
    "    click_possible_docs_tabs(driver)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # collect page text\n",
    "    text = \"\"\n",
    "    for sel in [\"main\", \"article\", \"#content\", \".content\", \".main\", \".container\", \".detail\", \".panel-body\"]:\n",
    "        els = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            t = els[0].get_attribute(\"innerText\").strip()\n",
    "            if len(t) > 80:\n",
    "                text = t; break\n",
    "    if not text:\n",
    "        text = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"innerText\").strip()\n",
    "\n",
    "    # collect & download PDFs\n",
    "    pdf_urls = collect_pdf_links(driver, driver.current_url)\n",
    "    ensure_dir(DOWNLOAD_DIR)\n",
    "    paths = []\n",
    "    for i, purl in enumerate(pdf_urls[:max_pdfs], 1):\n",
    "        try:\n",
    "            # request with Referer\n",
    "            r = req_sess.get(purl, headers={\"Referer\": detail_url}, timeout=timeout, allow_redirects=True)\n",
    "            ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            is_pdf = r.ok and (b\"%PDF\" in r.content[:1024] or \"application/pdf\" in ctype)\n",
    "            if is_pdf:\n",
    "                dest = os.path.join(DOWNLOAD_DIR, f\"{uid}_{i}.pdf\")\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                paths.append(dest)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return text, paths\n",
    "\n",
    "# ---------------- Master crawl ----------------\n",
    "def crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15):\n",
    "    driver = make_driver(headless=headless)\n",
    "    driver.get(START_URL)\n",
    "    accept_cookies(driver)\n",
    "    wait_for_table(driver)\n",
    "    set_page_size_max(driver)\n",
    "\n",
    "    headers = get_table_headers(driver)\n",
    "    rows, page = [], 1\n",
    "    while True:\n",
    "        print(f\"📄 Page {page} …\", end=\"\", flush=True)\n",
    "        page_rows = row_dicts_all_columns(driver, headers)\n",
    "        rows.extend(page_rows)\n",
    "        print(f\" kept {len(page_rows)}\")\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        time.sleep(per_page_delay)\n",
    "        if not go_next(driver):\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    # Assign stable UID per row (based on detail_url fallback to Bezeichnung)\n",
    "    for idx, r in enumerate(rows, 1):\n",
    "        base = r.get(\"detail_url\") or r.get(\"Bezeichnung\") or str(idx)\n",
    "        r[\"uid\"] = \"EVG-\" + uuid.uuid5(uuid.NAMESPACE_URL, base).hex[:12].upper()\n",
    "\n",
    "    # Prepare requests session with Selenium cookies\n",
    "    req_sess = selenium_cookies_to_requests(driver)\n",
    "\n",
    "    # Visit each detail, extract content, download ALL PDFs\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        durl = r.get(\"detail_url\", \"\")\n",
    "        if not durl:\n",
    "            r[\"detail_content\"] = \"\"\n",
    "            r[\"pdf_paths\"] = \"\"\n",
    "            continue\n",
    "        print(f\"   ↪ details [{i}/{len(rows)}]: {durl}\")\n",
    "        text, pdfs = fetch_detail_and_pdfs(driver, durl, r[\"uid\"], req_sess)\n",
    "        r[\"detail_content\"] = text\n",
    "        r[\"pdf_paths\"] = \" | \".join(pdfs) if pdfs else \"\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save CSV\n",
    "    all_keys = set(headers + [f\"{h}_link\" for h in headers] + [\"uid\",\"matched_keywords\",\"detail_url\",\"detail_content\",\"pdf_paths\"])\n",
    "    for r in rows: all_keys.update(r.keys())\n",
    "    preferred = [\"uid\",\"Bezeichnung\",\"Geschäftszeichen\",\"Vergabestelle\",\"Ort der Leistung\",\n",
    "                 \"matched_keywords\",\"detail_url\",\"pdf_paths\",\"detail_content\"]\n",
    "    header = [k for k in preferred if k in all_keys] + [k for k in sorted(all_keys) if k not in preferred]\n",
    "\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"evergabe_filtered_with_pdfs_{ts}.csv\"\n",
    "    with open(out, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "\n",
    "    print(f\"✅ Saved {len(rows)} rows to {out}\")\n",
    "    print(f\"📂 PDFs saved under: {os.path.abspath(DOWNLOAD_DIR)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # headless=False to watch; set max_pages for quick test\n",
    "    crawl_all_once(headless=True, max_pages=None, per_page_delay=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d78e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\damla\\appdata\\roaming\\python\\python313\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\damla\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f1550",
   "metadata": {},
   "source": [
    "PDV+CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b70fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Page 1 … kept 2\n",
      "📄 Page 2 … kept 5\n",
      "📄 Page 3 … kept 2\n",
      "📄 Page 4 … kept 2\n",
      "📄 Page 5 … kept 2\n",
      "📄 Page 6 … kept 1\n",
      "📄 Page 7 … kept 1\n",
      "📄 Page 8 … kept 1\n",
      "📄 Page 9 … kept 3\n",
      "📄 Page 10 … kept 2\n",
      "📄 Page 11 … kept 1\n",
      "📄 Page 12 … kept 2\n",
      "📄 Page 13 … kept 0\n",
      "📄 Page 14 … kept 2\n",
      "📄 Page 15 … kept 2\n",
      "📄 Page 16 … kept 2\n",
      "📄 Page 17 … kept 4\n",
      "📄 Page 18 … kept 4\n",
      "📄 Page 19 … kept 5\n",
      "📄 Page 20 … kept 2\n",
      "📄 Page 21 … kept 2\n",
      "📄 Page 22 … kept 4\n",
      "📄 Page 23 … kept 1\n",
      "📄 Page 24 … kept 3\n",
      "📄 Page 25 … kept 2\n",
      "📄 Page 26 … kept 3\n",
      "📄 Page 27 … kept 1\n",
      "📄 Page 28 … kept 1\n",
      "📄 Page 29 … kept 3\n",
      "📄 Page 30 … kept 1\n",
      "📄 Page 31 … kept 1\n",
      "📄 Page 32 … kept 0\n",
      "📄 Page 33 … kept 3\n",
      "📄 Page 34 … kept 4\n",
      "📄 Page 35 … kept 1\n",
      "📄 Page 36 … kept 2\n",
      "📄 Page 37 … kept 1\n",
      "📄 Page 38 … kept 1\n",
      "📄 Page 39 … kept 2\n",
      "📄 Page 40 … kept 2\n",
      "📄 Page 41 … kept 3\n",
      "📄 Page 42 … kept 3\n",
      "📄 Page 43 … kept 1\n",
      "📄 Page 44 … kept 2\n",
      "📄 Page 45 … kept 1\n",
      "📄 Page 46 … kept 3\n",
      "📄 Page 47 … kept 4\n",
      "📄 Page 48 … kept 0\n",
      "📄 Page 49 … kept 2\n",
      "📄 Page 50 … kept 0\n",
      "📄 Page 51 … kept 2\n",
      "📄 Page 52 … kept 0\n",
      "📄 Page 53 … kept 4\n",
      "📄 Page 54 … kept 2\n",
      "📄 Page 55 … kept 1\n",
      "📄 Page 56 … kept 1\n",
      "📄 Page 57 … kept 2\n",
      "📄 Page 58 … kept 1\n",
      "📄 Page 59 … kept 4\n",
      "📄 Page 60 … kept 3\n",
      "📄 Page 61 … kept 1\n",
      "📄 Page 62 … kept 3\n",
      "📄 Page 63 … kept 1\n",
      "📄 Page 64 … kept 1\n",
      "📄 Page 65 … kept 3\n",
      "📄 Page 66 … kept 2\n",
      "📄 Page 67 … kept 3\n",
      "📄 Page 68 … kept 3\n",
      "📄 Page 69 … kept 4\n",
      "📄 Page 70 … kept 2\n",
      "📄 Page 71 … kept 3\n",
      "📄 Page 72 … kept 1\n",
      "📄 Page 73 … kept 1\n",
      "📄 Page 74 … kept 1\n",
      "📄 Page 75 … kept 2\n",
      "📄 Page 76 … kept 1\n",
      "📄 Page 77 … kept 0\n",
      "📄 Page 78 … kept 3\n",
      "📄 Page 79 … kept 0\n",
      "📄 Page 80 … kept 3\n",
      "📄 Page 81 … kept 1\n",
      "📄 Page 82 … kept 3\n",
      "📄 Page 83 … kept 1\n",
      "📄 Page 84 … kept 2\n",
      "📄 Page 85 … kept 3\n",
      "📄 Page 86 … kept 3\n",
      "📄 Page 87 … kept 0\n",
      "📄 Page 88 … kept 2\n",
      "📄 Page 89 … kept 4\n",
      "📄 Page 90 … kept 5\n",
      "📄 Page 91 … kept 0\n",
      "📄 Page 92 … kept 4\n",
      "📄 Page 93 … kept 1\n",
      "📄 Page 94 … kept 2\n",
      "📄 Page 95 … kept 1\n",
      "📄 Page 96 … kept 0\n",
      "📄 Page 97 … kept 3\n",
      "📄 Page 98 … kept 5\n",
      "📄 Page 99 … kept 2\n",
      "📄 Page 100 … kept 3\n",
      "📄 Page 101 … kept 4\n",
      "📄 Page 102 … kept 5\n",
      "📄 Page 103 … kept 0\n",
      "📄 Page 104 … kept 0\n",
      "📄 Page 105 … kept 3\n",
      "📄 Page 106 … kept 1\n",
      "📄 Page 107 … kept 2\n",
      "📄 Page 108 … kept 4\n",
      "📄 Page 109 … kept 2\n",
      "📄 Page 110 … kept 0\n",
      "📄 Page 111 … kept 1\n",
      "📄 Page 112 … kept 3\n",
      "📄 Page 113 … kept 4\n",
      "📄 Page 114 … kept 2\n",
      "📄 Page 115 … kept 2\n",
      "📄 Page 116 … kept 1\n",
      "📄 Page 117 … kept 4\n",
      "📄 Page 118 … kept 2\n",
      "📄 Page 119 … kept 0\n",
      "📄 Page 120 … kept 2\n",
      "📄 Page 121 … kept 8\n",
      "📄 Page 122 … kept 0\n",
      "📄 Page 123 … kept 0\n",
      "📄 Page 124 … kept 3\n",
      "📄 Page 125 … kept 1\n",
      "📄 Page 126 … kept 3\n",
      "📄 Page 127 … kept 2\n",
      "📄 Page 128 … kept 1\n",
      "   ↪ [1/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802262\n",
      "   ↪ [2/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806331\n",
      "   ↪ [3/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806308\n",
      "   ↪ [4/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806284\n",
      "   ↪ [5/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806261\n",
      "   ↪ [6/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806253\n",
      "   ↪ [7/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806229\n",
      "   ↪ [8/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806215\n",
      "   ↪ [9/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806193\n",
      "   ↪ [10/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805884\n",
      "   ↪ [11/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805956\n",
      "   ↪ [12/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805994\n",
      "   ↪ [13/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805977\n",
      "   ↪ [14/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805899\n",
      "   ↪ [15/265] details: https://www.evergabe-online.de/tenderdetails.html?id=806068\n",
      "   ↪ [16/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805961\n",
      "   ↪ [17/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805927\n",
      "   ↪ [18/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805876\n",
      "   ↪ [19/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805684\n",
      "   ↪ [20/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805146\n",
      "   ↪ [21/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805814\n",
      "   ↪ [22/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805775\n",
      "   ↪ [23/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805617\n",
      "   ↪ [24/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805621\n",
      "   ↪ [25/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805427\n",
      "   ↪ [26/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805709\n",
      "   ↪ [27/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805711\n",
      "   ↪ [28/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805657\n",
      "   ↪ [29/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805594\n",
      "   ↪ [30/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805604\n",
      "   ↪ [31/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805523\n",
      "   ↪ [32/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805084\n",
      "   ↪ [33/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805478\n",
      "   ↪ [34/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805449\n",
      "   ↪ [35/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805436\n",
      "   ↪ [36/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799152\n",
      "   ↪ [37/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805187\n",
      "   ↪ [38/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805176\n",
      "   ↪ [39/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805095\n",
      "   ↪ [40/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805182\n",
      "   ↪ [41/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805154\n",
      "   ↪ [42/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801071\n",
      "   ↪ [43/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805195\n",
      "   ↪ [44/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805193\n",
      "   ↪ [45/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805149\n",
      "   ↪ [46/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805111\n",
      "   ↪ [47/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804886\n",
      "   ↪ [48/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804907\n",
      "   ↪ [49/265] details: https://www.evergabe-online.de/tenderdetails.html?id=805040\n",
      "   ↪ [50/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804863\n",
      "   ↪ [51/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804833\n",
      "   ↪ [52/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804745\n",
      "   ↪ [53/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804645\n",
      "   ↪ [54/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802109\n",
      "   ↪ [55/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804621\n",
      "   ↪ [56/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804955\n",
      "   ↪ [57/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804879\n",
      "   ↪ [58/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804748\n",
      "   ↪ [59/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804813\n",
      "   ↪ [60/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804786\n",
      "   ↪ [61/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804747\n",
      "   ↪ [62/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804564\n",
      "   ↪ [63/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804430\n",
      "   ↪ [64/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804448\n",
      "   ↪ [65/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804279\n",
      "   ↪ [66/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804610\n",
      "   ↪ [67/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804574\n",
      "   ↪ [68/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804389\n",
      "   ↪ [69/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804367\n",
      "   ↪ [70/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804158\n",
      "   ↪ [71/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804102\n",
      "   ↪ [72/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803949\n",
      "   ↪ [73/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804024\n",
      "   ↪ [74/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801429\n",
      "   ↪ [75/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804307\n",
      "   ↪ [76/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804267\n",
      "   ↪ [77/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804265\n",
      "   ↪ [78/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804187\n",
      "   ↪ [79/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803878\n",
      "   ↪ [80/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804069\n",
      "   ↪ [81/265] details: https://www.evergabe-online.de/tenderdetails.html?id=804056\n",
      "   ↪ [82/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803866\n",
      "   ↪ [83/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803890\n",
      "   ↪ [84/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802492\n",
      "   ↪ [85/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803718\n",
      "   ↪ [86/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803713\n",
      "   ↪ [87/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803938\n",
      "   ↪ [88/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802156\n",
      "   ↪ [89/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800785\n",
      "   ↪ [90/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803844\n",
      "   ↪ [91/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803720\n",
      "   ↪ [92/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803701\n",
      "   ↪ [93/265] details: https://www.evergabe-online.de/tenderdetails.html?id=794092\n",
      "   ↪ [94/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803390\n",
      "   ↪ [95/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801751\n",
      "   ↪ [96/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803468\n",
      "   ↪ [97/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803408\n",
      "   ↪ [98/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803207\n",
      "   ↪ [99/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801677\n",
      "   ↪ [100/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803482\n",
      "   ↪ [101/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803382\n",
      "   ↪ [102/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803359\n",
      "   ↪ [103/265] details: https://www.evergabe-online.de/tenderdetails.html?id=787829\n",
      "   ↪ [104/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803137\n",
      "   ↪ [105/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802897\n",
      "   ↪ [106/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803194\n",
      "   ↪ [107/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802848\n",
      "   ↪ [108/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803177\n",
      "   ↪ [109/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801501\n",
      "   ↪ [110/265] details: https://www.evergabe-online.de/tenderdetails.html?id=803143\n",
      "   ↪ [111/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802837\n",
      "   ↪ [112/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802646\n",
      "   ↪ [113/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802656\n",
      "   ↪ [114/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802886\n",
      "   ↪ [115/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802788\n",
      "   ↪ [116/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802678\n",
      "   ↪ [117/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802666\n",
      "   ↪ [118/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802477\n",
      "   ↪ [119/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802095\n",
      "   ↪ [120/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802436\n",
      "   ↪ [121/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802434\n",
      "   ↪ [122/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802355\n",
      "   ↪ [123/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802266\n",
      "   ↪ [124/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802449\n",
      "   ↪ [125/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800322\n",
      "   ↪ [126/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802403\n",
      "   ↪ [127/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802166\n",
      "   ↪ [128/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799621\n",
      "   ↪ [129/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802247\n",
      "   ↪ [130/265] details: https://www.evergabe-online.de/tenderdetails.html?id=802228\n",
      "   ↪ [131/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800745\n",
      "   ↪ [132/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801667\n",
      "   ↪ [133/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801671\n",
      "   ↪ [134/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799368\n",
      "   ↪ [135/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801727\n",
      "   ↪ [136/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801584\n",
      "   ↪ [137/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801748\n",
      "   ↪ [138/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801175\n",
      "   ↪ [139/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801514\n",
      "   ↪ [140/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801764\n",
      "   ↪ [141/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801330\n",
      "   ↪ [142/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801060\n",
      "   ↪ [143/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801617\n",
      "   ↪ [144/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801597\n",
      "   ↪ [145/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801439\n",
      "   ↪ [146/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800404\n",
      "   ↪ [147/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797598\n",
      "   ↪ [148/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801170\n",
      "   ↪ [149/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801369\n",
      "   ↪ [150/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801068\n",
      "   ↪ [151/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801082\n",
      "   ↪ [152/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800895\n",
      "   ↪ [153/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800878\n",
      "   ↪ [154/265] details: https://www.evergabe-online.de/tenderdetails.html?id=801139\n",
      "   ↪ [155/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796503\n",
      "   ↪ [156/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800807\n",
      "   ↪ [157/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800371\n",
      "   ↪ [158/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800898\n",
      "   ↪ [159/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799064\n",
      "   ↪ [160/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800802\n",
      "   ↪ [161/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800433\n",
      "   ↪ [162/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800529\n",
      "   ↪ [163/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800411\n",
      "   ↪ [164/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800436\n",
      "   ↪ [165/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800311\n",
      "   ↪ [166/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800441\n",
      "   ↪ [167/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800488\n",
      "   ↪ [168/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800030\n",
      "   ↪ [169/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799371\n",
      "   ↪ [170/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799994\n",
      "   ↪ [171/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799968\n",
      "   ↪ [172/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800039\n",
      "   ↪ [173/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800006\n",
      "   ↪ [174/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800028\n",
      "   ↪ [175/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799749\n",
      "   ↪ [176/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799887\n",
      "   ↪ [177/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799777\n",
      "   ↪ [178/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799783\n",
      "   ↪ [179/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800056\n",
      "   ↪ [180/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799567\n",
      "   ↪ [181/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800017\n",
      "   ↪ [182/265] details: https://www.evergabe-online.de/tenderdetails.html?id=800001\n",
      "   ↪ [183/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799932\n",
      "   ↪ [184/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799643\n",
      "   ↪ [185/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799718\n",
      "   ↪ [186/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799700\n",
      "   ↪ [187/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799147\n",
      "   ↪ [188/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799634\n",
      "   ↪ [189/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799548\n",
      "   ↪ [190/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799566\n",
      "   ↪ [191/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799423\n",
      "   ↪ [192/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799422\n",
      "   ↪ [193/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799169\n",
      "   ↪ [194/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799392\n",
      "   ↪ [195/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799264\n",
      "   ↪ [196/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799018\n",
      "   ↪ [197/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798284\n",
      "   ↪ [198/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799099\n",
      "   ↪ [199/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798793\n",
      "   ↪ [200/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799087\n",
      "   ↪ [201/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799108\n",
      "   ↪ [202/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798923\n",
      "   ↪ [203/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798710\n",
      "   ↪ [204/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799086\n",
      "   ↪ [205/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799061\n",
      "   ↪ [206/265] details: https://www.evergabe-online.de/tenderdetails.html?id=799044\n",
      "   ↪ [207/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798988\n",
      "   ↪ [208/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798975\n",
      "   ↪ [209/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798618\n",
      "   ↪ [210/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798754\n",
      "   ↪ [211/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798700\n",
      "   ↪ [212/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798655\n",
      "   ↪ [213/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798660\n",
      "   ↪ [214/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798573\n",
      "   ↪ [215/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798283\n",
      "   ↪ [216/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798204\n",
      "   ↪ [217/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798134\n",
      "   ↪ [218/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798039\n",
      "   ↪ [219/265] details: https://www.evergabe-online.de/tenderdetails.html?id=798005\n",
      "   ↪ [220/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797581\n",
      "   ↪ [221/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797444\n",
      "   ↪ [222/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797688\n",
      "   ↪ [223/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797562\n",
      "   ↪ [224/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795886\n",
      "   ↪ [225/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797071\n",
      "   ↪ [226/265] details: https://www.evergabe-online.de/tenderdetails.html?id=797027\n",
      "   ↪ [227/265] details: https://www.evergabe-online.de/tenderdetails.html?id=787892\n",
      "   ↪ [228/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796533\n",
      "   ↪ [229/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796489\n",
      "   ↪ [230/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796460\n",
      "   ↪ [231/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796288\n",
      "   ↪ [232/265] details: https://www.evergabe-online.de/tenderdetails.html?id=796239\n",
      "   ↪ [233/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795763\n",
      "   ↪ [234/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795761\n",
      "   ↪ [235/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795644\n",
      "   ↪ [236/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795615\n",
      "   ↪ [237/265] details: https://www.evergabe-online.de/tenderdetails.html?id=795362\n",
      "   ↪ [238/265] details: https://www.evergabe-online.de/tenderdetails.html?id=794535\n",
      "   ↪ [239/265] details: https://www.evergabe-online.de/tenderdetails.html?id=793812\n",
      "   ↪ [240/265] details: https://www.evergabe-online.de/tenderdetails.html?id=793725\n",
      "   ↪ [241/265] details: https://www.evergabe-online.de/tenderdetails.html?id=792801\n",
      "   ↪ [242/265] details: https://www.evergabe-online.de/tenderdetails.html?id=792929\n",
      "   ↪ [243/265] details: https://www.evergabe-online.de/tenderdetails.html?id=792416\n",
      "   ↪ [244/265] details: https://www.evergabe-online.de/tenderdetails.html?id=792367\n",
      "   ↪ [245/265] details: https://www.evergabe-online.de/tenderdetails.html?id=791668\n",
      "   ↪ [246/265] details: https://www.evergabe-online.de/tenderdetails.html?id=789881\n",
      "   ↪ [247/265] details: https://www.evergabe-online.de/tenderdetails.html?id=789816\n",
      "   ↪ [248/265] details: https://www.evergabe-online.de/tenderdetails.html?id=780730\n",
      "   ↪ [249/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788951\n",
      "   ↪ [250/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788955\n",
      "   ↪ [251/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788437\n",
      "   ↪ [252/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788141\n",
      "   ↪ [253/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788135\n",
      "   ↪ [254/265] details: https://www.evergabe-online.de/tenderdetails.html?id=788165\n",
      "   ↪ [255/265] details: https://www.evergabe-online.de/tenderdetails.html?id=787336\n",
      "   ↪ [256/265] details: https://www.evergabe-online.de/tenderdetails.html?id=778425\n",
      "   ↪ [257/265] details: https://www.evergabe-online.de/tenderdetails.html?id=776484\n",
      "   ↪ [258/265] details: https://www.evergabe-online.de/tenderdetails.html?id=764900\n",
      "   ↪ [259/265] details: https://www.evergabe-online.de/tenderdetails.html?id=746884\n",
      "   ↪ [260/265] details: https://www.evergabe-online.de/tenderdetails.html?id=669135\n",
      "   ↪ [261/265] details: https://www.evergabe-online.de/tenderdetails.html?id=583374\n",
      "   ↪ [262/265] details: https://www.evergabe-online.de/tenderdetails.html?id=531559\n",
      "   ↪ [263/265] details: https://www.evergabe-online.de/tenderdetails.html?id=521988\n",
      "   ↪ [264/265] details: https://www.evergabe-online.de/tenderdetails.html?id=503231\n",
      "   ↪ [265/265] details: https://www.evergabe-online.de/tenderdetails.html?id=386704\n",
      "\n",
      "✅ Saved 265 rows to evergabe_filtered_with_details_20251001_155506.csv\n",
      "📂 PDFs saved to: c:\\Users\\damla\\Desktop\\downloads\n"
     ]
    }
   ],
   "source": [
    "# evergabe_singlepass_full.py\n",
    "# Single-pass, fast scraper for evergabe-online.de:\n",
    "# - Multilingual inclusion & exclusion keyword filters\n",
    "# - Full column extraction (+ per-cell link URLs)\n",
    "# - Stable UID per row\n",
    "# - Visits each detail_url, scrapes page content, downloads ALL PDFs\n",
    "# - Saves CSV + PDFs under ./downloads\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import unicodedata\n",
    "import os\n",
    "import uuid\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------- Settings --------\n",
    "START_URL = \"https://www.evergabe-online.de/search.html?4\"\n",
    "DOWNLOAD_DIR = \"downloads\"     # PDFs will be saved here\n",
    "HEADLESS = True                # set to False to watch the browser\n",
    "MAX_PAGES = None               # set an int (e.g., 5) for quick testing\n",
    "PER_PAGE_DELAY = 0.15          # seconds; be polite but fast\n",
    "\n",
    "# -------- Keywords (DE/FR/IT/EN) --------\n",
    "RAW_INCLUDE = [\n",
    "    # Study / Studie\n",
    "    \"study\",\"studie\",\"étude\",\"etude\",\"studio\",\n",
    "    # Analysis / Analyse\n",
    "    \"analysis\",\"analyse\",\"analisi\",\n",
    "    # Economy / Wirtschaft / Ökonomie\n",
    "    \"economy\",\"economics\",\"wirtschaft\",\"ökonomie\",\"economie\",\"economia\",\n",
    "    # Benchmarking\n",
    "    \"benchmark\",\"benchmarking\",\n",
    "    # Wirtschaftsberatung / consulting\n",
    "    \"wirtschaftsberatung\",\"consulting\",\"conseil\",\"consulenza\",\n",
    "    # CPV & categories\n",
    "    \"72000000\",\"79300000\",\n",
    "    \"it-dienste\",\n",
    "    \"beratung, software-entwicklung, internet und hilfestellung\",\n",
    "    \"markt- und wirtschaftsforschung\",\"umfragen und statistiken\",\n",
    "    # Dienstleistung / services\n",
    "    \"dienstleistung\",\"service\",\"prestation\",\"prestazione\",\n",
    "    # Ausschreibung / tender\n",
    "    \"ausschreibung\",\"appel d'offres\",\"appels d'offres\",\"gara\",\"procurement\",\n",
    "    # Offenes Verfahren / open procedure\n",
    "    \"offenes verfahren\",\"procédure ouverte\",\"procedure ouverte\",\"procedura aperta\",\n",
    "    # BKS / Bundesamt für Statistik / OFS\n",
    "    \"bks\",\"bundesamt für statistik\",\"office fédéral de la statistique\",\n",
    "    \"office federal de la statistique\",\"ufficio federale di statistica\",\n",
    "    \"federal statistical office\",\"ofs\",\"bfs\",\n",
    "    # SECO\n",
    "    \"seco\",\"staatssekretariat für wirtschaft\",\n",
    "    \"secrétariat d'état à l'économie\",\"secretariat d'etat a l'economie\",\n",
    "    \"segretariato di stato dell'economia\",\n",
    "    # Cantons / regions\n",
    "    \"kanton zürich\",\"kanton zuerich\",\"canton de zurich\",\"cantone zurigo\",\n",
    "    \"kanton luzern\",\"canton de lucerne\",\"cantone lucerna\",\n",
    "    \"kanton\",\"canton\",\"cantone\",\n",
    "    \"region\",\"regionen\",\"région\",\"regione\",\n",
    "    # Bundesamt für.. (generic)\n",
    "    \"bundesamt für\",\"bundesamt fuer\",\n",
    "    # BBL\n",
    "    \"bundesamt für bauten und logistik\",\"bundesamt fuer bauten und logistik\",\"bbl\",\n",
    "    # Index\n",
    "    \"index\",\"verbraucherindex\",\"price index\",\"indice des prix\",\"indice dei prezzi\",\n",
    "    # Wirtschaftsforschung\n",
    "    \"wirtschaftsforschung\",\"economic research\",\"recherche économique\",\n",
    "    \"recherche economique\",\"ricerca economica\",\n",
    "]\n",
    "\n",
    "RAW_EXCLUDE = [\n",
    "    \"construction\",\"bau\",\"bâtiment\",\"batiment\",\"costruzioni\",\n",
    "    \"health care\",\"gesundheit\",\"santé\",\"sante\",\"sanità\",\"sanita\",\n",
    "    \"transport\",\"verkehr\",\"transports\",\"trasporti\",\n",
    "    \"mobility\",\"mobilität\",\"mobilite\",\"mobilité\",\"mobilita\",\n",
    "    \"sport\",\n",
    "    \"culture\",\"kultur\",\"cultura\",\n",
    "    \"street\",\"strasse\",\"straße\",\"rue\",\"strada\",\n",
    "    \"infrastructure\",\"infrastruktur\",\"infrastruttura\",\n",
    "    \"process\",\"prozess\",\"processus\",\"processo\",\n",
    "    \"it\",\"informatik\",\"informatique\",\"informatica\",\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip accents, collapse whitespace.\"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "INCLUDE = {_norm(k) for k in RAW_INCLUDE if k.strip()}\n",
    "EXCLUDE = {_norm(k) for k in RAW_EXCLUDE if k.strip()}\n",
    "\n",
    "# -------- Selenium setup --------\n",
    "def make_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--lang=de-DE\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "def accept_cookies(driver):\n",
    "    labels = [\"Alle akzeptieren\",\"Akzeptieren\",\"Zustimmen\",\"Einverstanden\",\"OK\",\"Okay\",\"Ich stimme zu\"]\n",
    "    for label in labels:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//button[contains(., '{label}')]\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def wait_for_table(driver, timeout=25):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table tbody tr\"))\n",
    "    )\n",
    "\n",
    "def first_cell_token(driver):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"table tbody tr td\").get_attribute(\"innerText\").strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def set_page_size_max(driver):\n",
    "    \"\"\"Try to switch page size to the largest (100/200/Alle).\"\"\"\n",
    "    tried = False\n",
    "    for sel in [\"select\", \"select.page-size\", \"select[name*='size' i]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "            s = Select(el)\n",
    "            labels = [o.text.strip() or o.get_attribute(\"value\") for o in s.options]\n",
    "            target = None\n",
    "            for want in (\"200\",\"100\",\"Alle\",\"Alles\"):\n",
    "                for lbl in labels:\n",
    "                    if want in (lbl or \"\"):\n",
    "                        target = lbl; break\n",
    "                if target: break\n",
    "            if not target and labels:\n",
    "                target = labels[-1]\n",
    "            if target:\n",
    "                old = first_cell_token(driver)\n",
    "                s.select_by_visible_text(target)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                tried = True\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not tried:\n",
    "        for want in (\"200\",\"100\"):\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[normalize-space()='{want}']\")\n",
    "                old = first_cell_token(driver)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != old)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# -------- Table header & row extraction --------\n",
    "def unique_headers(headers):\n",
    "    counts = defaultdict(int)\n",
    "    out = []\n",
    "    for h in headers:\n",
    "        key = h or \"col\"\n",
    "        counts[key] += 1\n",
    "        out.append(key if counts[key] == 1 else f\"{key}_{counts[key]}\")\n",
    "    return out\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    ths = table.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "    headers = [th.get_attribute(\"innerText\").strip() or th.get_attribute(\"title\") or th.get_attribute(\"aria-label\") or \"\" for th in ths]\n",
    "    if not any(h for h in headers):\n",
    "        # fall back to data-title/aria-label on tds\n",
    "        try:\n",
    "            first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "            tds = first_row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "            tmp = []\n",
    "            for td in tds:\n",
    "                label = td.get_attribute(\"data-title\") or td.get_attribute(\"aria-label\") or \"\"\n",
    "                tmp.append(label)\n",
    "            headers = tmp\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not headers:\n",
    "        first_row = table.find_element(By.CSS_SELECTOR, \"tbody tr\")\n",
    "        n = len(first_row.find_elements(By.CSS_SELECTOR, \"td\"))\n",
    "        headers = [f\"col_{i+1}\" for i in range(n)]\n",
    "    return unique_headers([h.strip() for h in headers])\n",
    "\n",
    "def row_dicts_all_columns(driver, headers):\n",
    "    \"\"\"\n",
    "    Extract all visible columns and apply inclusion/exclusion filters.\n",
    "    Adds <column>_link if a link exists in that cell.\n",
    "    \"\"\"\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "    rows = []\n",
    "    for tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "        tds = tr.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "        if not tds:\n",
    "            continue\n",
    "        row = {}\n",
    "        concat = []\n",
    "        for i, td in enumerate(tds):\n",
    "            key = headers[i] if i < len(headers) else f\"col_{i+1}\"\n",
    "            text = td.get_attribute(\"innerText\").strip()\n",
    "            row[key] = text\n",
    "            concat.append(text)\n",
    "            # capture first link in cell\n",
    "            try:\n",
    "                a = td.find_element(By.CSS_SELECTOR, \"a[href]\")\n",
    "                row[f\"{key}_link\"] = a.get_attribute(\"href\")\n",
    "                if i == 0 and \"detail_url\" not in row:\n",
    "                    row[\"detail_url\"] = a.get_attribute(\"href\")  # likely the main detail link\n",
    "            except Exception:\n",
    "                pass\n",
    "        norm = _norm(\" | \".join(concat))\n",
    "        hits_incl = sorted({kw for kw in INCLUDE if kw in norm})\n",
    "        hits_excl = sorted({kw for kw in EXCLUDE if kw in norm})\n",
    "        if hits_incl and not hits_excl:\n",
    "            row[\"matched_keywords\"] = \", \".join(hits_incl)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def go_next(driver):\n",
    "    try:\n",
    "        nxt = driver.find_element(By.CSS_SELECTOR, \"a[rel='next']\")\n",
    "        if (nxt.get_attribute(\"aria-disabled\") or \"\").lower() == \"true\":\n",
    "            return False\n",
    "        token = first_cell_token(driver)\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        WebDriverWait(driver, 15).until(lambda d: first_cell_token(d) != token)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------- Detail page scraping & PDFs --------\n",
    "def ensure_dir(path):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def selenium_cookies_to_requests(driver) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    for c in driver.get_cookies():\n",
    "        s.cookies.set(c[\"name\"], c[\"value\"], domain=c.get(\"domain\"))\n",
    "    s.headers.update({\"User-Agent\": \"Mozilla/5.0 (compatible; EvergabeScraper/1.0)\"})\n",
    "    return s\n",
    "\n",
    "def collect_pdf_links(driver, base_url):\n",
    "    \"\"\"Find ALL candidate PDF links on the current page.\"\"\"\n",
    "    found = set()\n",
    "    # direct anchors with .pdf or type\n",
    "    for a in driver.find_elements(By.CSS_SELECTOR, \"a[href]\"):\n",
    "        href = a.get_attribute(\"href\") or \"\"\n",
    "        text = (a.text or \"\").lower()\n",
    "        typ = (a.get_attribute(\"type\") or \"\").lower()\n",
    "        if \".pdf\" in href.lower() or \"pdf\" in text or \"application/pdf\" in typ:\n",
    "            found.add(urljoin(base_url, href))\n",
    "    # <embed> or <object>\n",
    "    for e in driver.find_elements(By.CSS_SELECTOR, \"embed[src], object[data]\"):\n",
    "        src = e.get_attribute(\"src\") or e.get_attribute(\"data\") or \"\"\n",
    "        if \".pdf\" in (src or \"\").lower():\n",
    "            found.add(urljoin(base_url, src))\n",
    "    return sorted(found)\n",
    "\n",
    "def click_possible_docs_tabs(driver):\n",
    "    \"\"\"Try to reveal document tabs/sections.\"\"\"\n",
    "    labels = [\"Dokumente\",\"Vergabeunterlagen\",\"Unterlagen\",\"Documents\",\"Documenti\",\"Documents\",\"Downloads\"]\n",
    "    for lab in labels:\n",
    "        try:\n",
    "            el = driver.find_element(By.XPATH, f\"//a[contains(., '{lab}')]|//button[contains(., '{lab}')]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", el)\n",
    "            time.sleep(0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def fetch_detail_and_pdfs(driver, detail_url, uid, req_sess, max_pdfs=10, timeout=30):\n",
    "    \"\"\"Open detail page, extract visible text, download ALL PDFs. Returns (text, [paths]).\"\"\"\n",
    "    try:\n",
    "        driver.get(detail_url)\n",
    "        WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "        time.sleep(0.8)  # let JS render if needed\n",
    "    except Exception:\n",
    "        return \"\", []\n",
    "\n",
    "    # Open document tabs if any\n",
    "    click_possible_docs_tabs(driver)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    # Extract main text\n",
    "    text = \"\"\n",
    "    for sel in [\"main\", \"article\", \"#content\", \".content\", \".main\", \".container\", \".detail\", \".panel-body\", \".modul-detailansicht\"]:\n",
    "        els = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            t = els[0].get_attribute(\"innerText\").strip()\n",
    "            if len(t) > 50:\n",
    "                text = t\n",
    "                break\n",
    "    if not text:\n",
    "        try:\n",
    "            text = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"innerText\").strip()\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "\n",
    "    # Collect & download PDFs\n",
    "    pdf_urls = collect_pdf_links(driver, driver.current_url)\n",
    "    ensure_dir(DOWNLOAD_DIR)\n",
    "    paths = []\n",
    "    for i, purl in enumerate(pdf_urls[:max_pdfs], 1):\n",
    "        try:\n",
    "            r = req_sess.get(purl, headers={\"Referer\": detail_url}, timeout=timeout, allow_redirects=True)\n",
    "            ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            is_pdf = r.ok and (b\"%PDF\" in r.content[:1024] or \"application/pdf\" in ctype)\n",
    "            if is_pdf:\n",
    "                dest = os.path.join(DOWNLOAD_DIR, f\"{uid}_{i}.pdf\")\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                paths.append(dest)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return text, paths\n",
    "\n",
    "# -------- Master crawl --------\n",
    "def crawl_all_once(headless=HEADLESS, max_pages=MAX_PAGES, per_page_delay=PER_PAGE_DELAY):\n",
    "    driver = make_driver(headless=headless)\n",
    "    driver.get(START_URL)\n",
    "    accept_cookies(driver)\n",
    "    wait_for_table(driver)\n",
    "    set_page_size_max(driver)\n",
    "\n",
    "    # Build headers once (fastest for consistent table)\n",
    "    headers = get_table_headers(driver)\n",
    "\n",
    "    rows = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"📄 Page {page} …\", end=\"\", flush=True)\n",
    "        page_rows = row_dicts_all_columns(driver, headers)\n",
    "        rows.extend(page_rows)\n",
    "        print(f\" kept {len(page_rows)}\")\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        time.sleep(per_page_delay)\n",
    "        if not go_next(driver):\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    # Assign stable UID per row (based on detail_url, fallback to Bezeichnung)\n",
    "    for idx, r in enumerate(rows, 1):\n",
    "        base = r.get(\"detail_url\") or r.get(\"Bezeichnung\") or str(idx)\n",
    "        r[\"uid\"] = \"EVG-\" + uuid.uuid5(uuid.NAMESPACE_URL, base).hex[:12].upper()\n",
    "\n",
    "    # Prepare requests session with Selenium cookies (for downloads)\n",
    "    req_sess = selenium_cookies_to_requests(driver)\n",
    "\n",
    "    # Visit each detail_url, extract content, download PDFs\n",
    "    total = len(rows)\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        durl = r.get(\"detail_url\", \"\")\n",
    "        if not durl:\n",
    "            r[\"detail_content\"] = \"\"\n",
    "            r[\"pdf_paths\"] = \"\"\n",
    "            print(f\"   ↪ [{i}/{total}] no detail_url\")\n",
    "            continue\n",
    "        print(f\"   ↪ [{i}/{total}] details: {durl}\")\n",
    "        text, pdfs = fetch_detail_and_pdfs(driver, durl, r[\"uid\"], req_sess)\n",
    "        r[\"detail_content\"] = text\n",
    "        r[\"pdf_paths\"] = \" | \".join(pdfs) if pdfs else \"\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save CSV\n",
    "    all_keys = set(headers + [f\"{h}_link\" for h in headers] + [\"uid\",\"matched_keywords\",\"detail_url\",\"detail_content\",\"pdf_paths\"])\n",
    "    for r in rows:\n",
    "        all_keys.update(r.keys())\n",
    "\n",
    "    preferred = [\n",
    "        \"uid\",\"Bezeichnung\",\"Geschäftszeichen\",\"Vergabestelle\",\"Ort der Leistung\",\n",
    "        \"matched_keywords\",\"detail_url\",\"pdf_paths\",\"detail_content\"\n",
    "    ]\n",
    "    header = [k for k in preferred if k in all_keys] + [k for k in sorted(all_keys) if k not in preferred]\n",
    "\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"evergabe_filtered_with_details_{ts}.csv\"\n",
    "    with open(out, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"\\n✅ Saved {len(rows)} rows to {out}\")\n",
    "    print(f\"📂 PDFs saved to: {os.path.abspath(DOWNLOAD_DIR)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_all_once()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
