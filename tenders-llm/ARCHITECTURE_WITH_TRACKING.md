# Tenders-LLM Architecture with Experiment Tracking

## Overview

Complete MLOps-ready architecture for tracking experiments, managing data dependencies, and deploying to production.

---

## System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA SOURCES                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚                    â”‚
    â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SIMAP   â”‚      â”‚  Services   â”‚      â”‚  Keywords    â”‚
â”‚ Tenders â”‚      â”‚  (37 items) â”‚      â”‚  (Optional)  â”‚
â”‚ 4,748   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                    â”‚
    â”‚                    â”‚                    â”‚
    â”‚                    â–¼                    â–¼
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚ EXPERIMENT TRACKER           â”‚
    â”‚            â”‚ - Compute hashes             â”‚
    â”‚            â”‚ - Detect changes             â”‚
    â”‚            â”‚ - Version metadata           â”‚
    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚
    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLFLOW TRACKING                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Parameters Logged:                                         â”‚
â”‚  âœ“ data.total_tenders         = 4748                       â”‚
â”‚  âœ“ data.positive_tenders      = 162                        â”‚
â”‚  âœ“ data.data_hash             = a3f5c2...                  â”‚
â”‚  âœ“ services.services_count    = 37                         â”‚
â”‚  âœ“ services.services_hash     = b8d4e1...                  â”‚
â”‚  âœ“ keywords.keywords_count    = 12                         â”‚
â”‚  âœ“ keywords.keywords_hash     = c7a3f2...                  â”‚
â”‚  âœ“ prompt.prompt_hash         = e4f1a8...                  â”‚
â”‚  âœ“ input.use_full_text        = False                      â”‚
â”‚  âœ“ model.model_name           = gpt-4o-mini                â”‚
â”‚  âœ“ model.temperature          = 0.1                        â”‚
â”‚                                                             â”‚
â”‚  Metrics Logged:                                            â”‚
â”‚  âœ“ val.accuracy              = 0.92                        â”‚
â”‚  âœ“ val.precision             = 0.95                        â”‚
â”‚  âœ“ val.recall                = 0.87                        â”‚
â”‚  âœ“ val.f1_score              = 0.909                       â”‚
â”‚  âœ“ val.pr_auc                = 0.91                        â”‚
â”‚  âœ“ test.accuracy             = ...                         â”‚
â”‚  âœ“ api_cost_usd              = 5.50                        â”‚
â”‚                                                             â”‚
â”‚  Artifacts Logged:                                          â”‚
â”‚  âœ“ prompts/classify_tender.md                              â”‚
â”‚  âœ“ data/raw/services.md                                    â”‚
â”‚  âœ“ data/raw/keywords.csv                                   â”‚
â”‚  âœ“ data/processed/preds_val.jsonl                          â”‚
â”‚  âœ“ reports/pr_curve_val.png                                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  MLflow UI            â”‚
            â”‚  http://localhost:5000â”‚
            â”‚                       â”‚
            â”‚  - Compare experimentsâ”‚
            â”‚  - View artifacts     â”‚
            â”‚  - Track costs        â”‚
            â”‚  - Promote to prod    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Tracking Details

### What Triggers "Data Changed"

The `data_hash` parameter changes when:

âœ… **New tenders added** (SIMAP updates)
```bash
# Before: 4,748 tenders, hash: a3f5c2...
python scripts/00_reload_data_with_fulltext.py
# After:  5,000 tenders, hash: d7e9f1... â† CHANGED
```

âœ… **Labels updated** (manual corrections)
```bash
# Someone updates selected_ids.csv
# Hash changes: a3f5c2... â†’ b4g6d3...
```

âœ… **Splits changed** (re-run 02_make_splits.py)
```bash
# Train/val/test IDs change
# Hash changes because split composition changed
```

### What Triggers "Services Changed"

The `services_hash` changes when:

âœ… **Services.xlsx updated**
```bash
# Add new service or edit description
# Hash changes: b8d4e1... â†’ c9f2d3...
```

âœ… **services.md created/updated**
```bash
# Convert from Excel or manually edit
# Hash changes: b8d4e1... â†’ c9f2d3...
```

### What Triggers "Keywords Changed"

The `keywords_hash` changes when:

âœ… **keywords.csv updated**
```bash
# Add/remove/edit keywords
# Hash changes: c7a3f2... â†’ d8g4h4...
```

### What Triggers "Prompt Changed"

The `prompt_hash` changes when:

âœ… **Services change** â†’ Prompt rebuilt
âœ… **Keywords change** â†’ Prompt rebuilt
âœ… **Training data changes** â†’ Different few-shot examples
âœ… **Manual prompt edits**

---

## Experiment Tracking Workflow

### 1. Development Loop

```bash
# A. Update something (services, keywords, data)
nano data/raw/services.md

# B. Rebuild dependencies
python scripts/03_build_prompt.py

# C. Run experiment with tracking
export EXPERIMENT_NAME="exp_$(date +%Y%m%d_%H%M%S)"
export EXPERIMENT_DESCRIPTION="Updated Datenportal service"

python scripts/04_llm_predict.py
python scripts/05_eval_with_mlflow.py
```

**MLflow automatically logs:**
- What changed (hashes)
- New metrics
- All configuration
- Artifacts (files)

### 2. Comparison

```bash
# In MLflow UI
# Select: exp_baseline + exp_services_v2
# Click: Compare
```

**See exactly what differs:**
```
Parameters:
  services.services_hash:  b8d4e1... â†’ c9f2d3...  [CHANGED]
  data.data_hash:          a3f5c2... â†’ a3f5c2...  [SAME]
  
Metrics:
  val.accuracy:            0.92 â†’ 0.93  [+1%]
  val.precision:           0.95 â†’ 0.96  [+1%]
  val.recall:              0.87 â†’ 0.88  [+1%]
```

### 3. Production Deployment

```python
# Promote best experiment to production
import mlflow

client = mlflow.tracking.MlflowClient()

# Register model
mlflow.register_model(
    f"runs:/{run_id}/model",
    "tender_classifier"
)

# Transition to production
client.transition_model_version_stage(
    name="tender_classifier",
    version=3,  # The best experiment
    stage="Production"
)
```

### 4. Production Monitoring

```python
# Daily batch job
with mlflow.start_run(run_name=f"production_{date}"):
    # Track what's running
    mlflow.log_param("deployed_version", "v3")
    mlflow.log_param("deployed_prompt_hash", "e4f1a8...")
    mlflow.log_param("deployed_services_hash", "c9f2d3...")
    
    # Track performance
    mlflow.log_metric("daily_predictions", 150)
    mlflow.log_metric("daily_positives", 8)
    mlflow.log_metric("precision_estimated", 0.94)
    mlflow.log_metric("api_cost_usd", 12.50)
```

---

## Complete Example: Dataset Update Scenario

### Scenario: SIMAP delivers new October 2025 tenders

#### Step 1: Baseline (Before New Data)

```bash
export EXPERIMENT_NAME="exp_baseline_sept2025"
python scripts/05_eval_with_mlflow.py
```

**Tracked:**
```yaml
data:
  tenders_total: 4748
  tenders_positive: 162
  positive_rate: 0.034
  data_hash: a3f5c2abc...
  date_range: "2008-10-30 to 2025-09-30"
  
services:
  services_count: 37
  services_hash: b8d4e1def...
  
metrics:
  val.accuracy: 0.92
  val.precision: 0.95
  val.recall: 0.87
```

#### Step 2: New Data Arrives

```bash
# Download new SIMAP data
# Update tenders_content.xlsx with Oct 2025 tenders

python scripts/00_reload_data_with_fulltext.py
# Output: Loaded 5000 tenders (was 4748)

python scripts/01_prepare_data.py
# Output: 180 positives (was 162)

python scripts/02_make_splits.py
# Output: New train/val/test splits
```

#### Step 3: Re-run with New Data

```bash
export EXPERIMENT_NAME="exp_with_oct2025_data"
export EXPERIMENT_DESCRIPTION="Re-ran with October 2025 SIMAP data (252 new tenders)"

python scripts/03_build_prompt.py  # Rebuilds with new train examples
python scripts/04_llm_predict.py
python scripts/05_eval_with_mlflow.py
```

**Tracked:**
```yaml
data:
  tenders_total: 5000  â† CHANGED
  tenders_positive: 180  â† CHANGED
  positive_rate: 0.036  â† CHANGED
  data_hash: d7e9f1ghi...  â† CHANGED (different data!)
  date_range: "2008-10-30 to 2025-10-31"  â† CHANGED
  train_size: 3750  â† CHANGED
  
services:
  services_count: 37  â† SAME
  services_hash: b8d4e1def...  â† SAME
  
prompt:
  prompt_hash: f5b2c9jkl...  â† CHANGED (new few-shot examples)
  
metrics:
  val.accuracy: 0.91  â† Slightly worse
  val.precision: 0.94  â† Slightly worse
  val.recall: 0.88  â† Improved!
```

#### Step 4: Analysis

**In MLflow UI, compare the two experiments:**

**What Changed:**
- âœ… Data: 252 new tenders added
- âœ… Training set: 189 new training examples
- âœ… Few-shot examples: Different mix due to new train set
- âŒ Services: No change
- âŒ Keywords: No change
- âŒ Model config: No change

**Performance Impact:**
- Accuracy: 92% â†’ 91% (-1%)
- Precision: 95% â†’ 94% (-1%)
- Recall: 87% â†’ 88% (+1%)

**Conclusion:**
- Slight performance degradation acceptable
- New data brings different distribution
- Consider rebalancing or updating prompt

**Decision:**
- Keep new data
- Update EXPERIMENT_LOG.md
- Monitor next batch

---

## File Structure

```
tenders-llm/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiment_tracker.py       # Hash computation, metadata extraction
â”‚   â””â”€â”€ mlflow_wrapper.py           # MLflow integration
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 00_reload_data_with_fulltext.py
â”‚   â”œâ”€â”€ 01_prepare_data.py
â”‚   â”œâ”€â”€ 02_make_splits.py
â”‚   â”œâ”€â”€ 03_build_prompt.py
â”‚   â”œâ”€â”€ 04_llm_predict.py
â”‚   â”œâ”€â”€ 05_eval.py                  # Original (no tracking)
â”‚   â””â”€â”€ 05_eval_with_mlflow.py      # NEW: With full tracking
â”‚
â”œâ”€â”€ experiments/                     # JSON configs
â”‚   â”œâ”€â”€ exp_baseline/
â”‚   â”‚   â””â”€â”€ experiment_config.json  # Complete snapshot
â”‚   â”œâ”€â”€ exp_services_v2/
â”‚   â”‚   â””â”€â”€ experiment_config.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ mlruns/                          # MLflow data
â”‚   â”œâ”€â”€ mlflow.db                    # SQLite database
â”‚   â””â”€â”€ 0/                           # Experiment artifacts
â”‚       â”œâ”€â”€ abc123.../               # Run ID
â”‚       â”‚   â”œâ”€â”€ artifacts/
â”‚       â”‚   â”‚   â”œâ”€â”€ prompt/
â”‚       â”‚   â”‚   â”œâ”€â”€ services/
â”‚       â”‚   â”‚   â”œâ”€â”€ predictions_val/
â”‚       â”‚   â”‚   â””â”€â”€ plots/
â”‚       â”‚   â””â”€â”€ meta.yaml
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ MLFLOW_GUIDE.md                  # Full documentation
â”œâ”€â”€ EXPERIMENT_TRACKING_QUICKSTART.md # Quick start guide
â””â”€â”€ ARCHITECTURE_WITH_TRACKING.md    # This file
```

---

## Benefits Summary

### For Development

âœ… **Know exactly what changed** between experiments
âœ… **Reproduce any experiment** using config snapshots
âœ… **Track costs** over time
âœ… **Compare prompts** side-by-side
âœ… **Detect data drift** via hash changes

### For Production

âœ… **Model registry** (version control for prompts)
âœ… **Staging â†’ Production** promotion workflow
âœ… **Rollback capability** if new version underperforms
âœ… **Performance monitoring** over time
âœ… **Cost tracking** for budget management
âœ… **A/B testing** (route 50% to v1, 50% to v2)

### For Team Collaboration

âœ… **Shared experiment database**
âœ… **Documented changes** (not just in heads)
âœ… **Onboarding** (new team members see history)
âœ… **Auditing** (who ran what, when)

---

## Next Steps

1. **Install MLflow:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Start UI:**
   ```bash
   mlflow ui --backend-store-uri sqlite:///mlruns/mlflow.db
   ```

3. **Run first tracked experiment:**
   ```bash
   export EXPERIMENT_NAME="exp_first_test"
   python scripts/05_eval_with_mlflow.py
   ```

4. **View in UI:**
   Open http://localhost:5000

5. **Read guides:**
   - Quick start: `EXPERIMENT_TRACKING_QUICKSTART.md`
   - Full docs: `MLFLOW_GUIDE.md`

---

**You now have production-grade experiment tracking that monitors:**
- âœ… Dataset changes (new tenders, label updates)
- âœ… Services changes (description updates)
- âœ… Keywords changes (keyword additions/removals)
- âœ… Prompt changes (any modifications)
- âœ… Input config changes (full text vs title)
- âœ… Model config changes (model, temperature)
- âœ… Performance metrics (accuracy, precision, recall)
- âœ… Costs (API usage tracking)

**Ready for production deployment with full traceability!** ğŸš€

