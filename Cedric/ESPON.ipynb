{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE verified: https://www.espon.eu\n",
            "   GET: https://www.espon.eu/engage/procurements\n",
            "   link(detail): Open PIN - [WEBHOUSE] An ESPON barometer on European local housing markets Annou -> https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <div cl\n",
            "   link(detail): Closed Prior Information Notice (PIN) for 4 Targeted Analysis Projects Announcem -> https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <div clas\n",
            "   link(detail): Open Call for expression of interest to establish a pool of editors to develop c -> https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <di\n",
            "   link(detail): Open [SECC] - Spill-over effects of European capitals of culture on territorial  -> https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\"> <div class=\"views-row\"> <div class=\"engage-status-wra\n",
            "Page 1: 4 candidate links (listing)\n",
            " -> Detail: https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\n",
            "   GET: https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\n",
            "    title fallback from anchor: Open PIN - [WEBHOUSE] An ESPON barometer on European local housing markets Announcements 05 June 203\n",
            " -> Detail: https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\n",
            "   GET: https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\n",
            "    title fallback from anchor: Closed Prior Information Notice (PIN) for 4 Targeted Analysis Projects Announcements 02 June 2030\n",
            " -> Detail: https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\n",
            "   GET: https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\n",
            "    title fallback from anchor: Open Call for expression of interest to establish a pool of editors to develop content Calls 20 Sept\n",
            " -> Detail: https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\n",
            "   GET: https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\n",
            "    title fallback from anchor: Open [SECC] - Spill-over effects of European capitals of culture on territorial development Calls 21\n",
            "Page 1: kept 4 tenders\n",
            "Wrote 4 rows to /Users/cedricjansen/Desktop/bak-economics/Cedric/espon_tenders.csv\n"
          ]
        }
      ],
      "source": [
        "import re, os, io, requests, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, parse_qs\n",
        "from datetime import datetime\n",
        "\n",
        "BASE = \"https://www.espon.eu\"\n",
        "URL = \"https://www.espon.eu/engage/procurements\"\n",
        "OUT = \"/Users/cedricjansen/Desktop/bak-economics/Cedric/espon_tenders.csv\"\n",
        "CUTOFF = datetime(2018, 1, 1)\n",
        "MAX_PAGES = 20  # hard cap; we will stop earlier if no more links\n",
        "\n",
        "print(f\"BASE verified: {BASE}\")\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "KEYWORDS = [\"procure\", \"tender\", \"call\", \"contract\", \"rfp\", \"rft\", \"rfq\", \"invitation\"]\n",
        "\n",
        "# Optional PDF text extraction\n",
        "pdf_text = None\n",
        "try:\n",
        "    from pypdf import PdfReader\n",
        "    def pdf_text(content: bytes) -> str:\n",
        "        try:\n",
        "            r = PdfReader(io.BytesIO(content))\n",
        "            return \"\\n\".join([(p.extract_text() or \"\") for p in r.pages])\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "def fetch(url: str, timeout: int = 25) -> bytes:\n",
        "    try:\n",
        "        log(f\"   GET: {url}\")\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        return r.content\n",
        "    except Exception as e:\n",
        "        log(f\"   GET FAILED: {url} -> {e}\")\n",
        "        return b\"\"\n",
        "\n",
        "\n",
        "def parse_date_any(text: str) -> datetime | None:\n",
        "    if not text:\n",
        "        return None\n",
        "    patterns = [\n",
        "        r\"\\b(\\d{1,2})[./-](\\d{1,2})[./-](\\d{2,4})\\b\",  # 20/09/2028 or 20-09-2028\n",
        "        r\"\\b(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+(\\d{2,4})\\b\",  # 20 Sep 2028\n",
        "        r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2}),\\s*(\\d{2,4})\\b\",  # September 20, 2028\n",
        "    ]\n",
        "    months = {\n",
        "        'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'sept':9,'oct':10,'nov':11,'dec':12,\n",
        "        'january':1,'february':2,'march':3,'april':4,'may':5,'june':6,'july':7,'august':8,'september':9,'october':10,'november':11,'december':12\n",
        "    }\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, text, flags=re.I)\n",
        "        if not m:\n",
        "            continue\n",
        "        g = list(m.groups())\n",
        "        # Case 1: d/m/y numeric\n",
        "        if g[0].isdigit() and g[1].isdigit():\n",
        "            d, mth, y = int(g[0]), int(g[1]), int(g[2])\n",
        "            if y < 100: y += 2000\n",
        "            return datetime(y, mth, d)\n",
        "        # Case 2: d Month y (e.g., 20 Sep 2028)\n",
        "        if g[0].isdigit() and not g[1].isdigit():\n",
        "            d, mname, y = int(g[0]), g[1].lower(), int(g[2])\n",
        "            if y < 100: y += 2000\n",
        "            mth = months.get(mname[:3], months.get(mname))\n",
        "            if mth:\n",
        "                return datetime(y, mth, d)\n",
        "        # Case 3: Month d, y (e.g., September 20, 2028)\n",
        "        if not g[0].isdigit() and g[1].isdigit():\n",
        "            mname, d, y = g[0].lower(), int(g[1]), int(g[2])\n",
        "            if y < 100: y += 2000\n",
        "            mth = months.get(mname[:3], months.get(mname))\n",
        "            if mth:\n",
        "                return datetime(y, mth, d)\n",
        "    return None\n",
        "\n",
        "\n",
        "def norm_date(text: str) -> str:\n",
        "    d = parse_date_any(text)\n",
        "    return d.strftime(\"%Y-%m-%d\") if d else \"\"\n",
        "\n",
        "\n",
        "def detect_languages(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"en\"\n",
        "    langs = []\n",
        "    t = text.lower()\n",
        "    if any(k in t for k in [\"deutsch\",\"german\",\"de \",\"(de)\"]): langs.append(\"de\")\n",
        "    if any(k in t for k in [\"franÃ§ais\",\"french\",\"fr \",\"(fr)\"]): langs.append(\"fr\")\n",
        "    if any(k in t for k in [\"italiano\",\"italian\",\"it \",\"(it)\"]): langs.append(\"it\")\n",
        "    if any(k in t for k in [\"english\",\"en \",\"(en)\"]): langs.append(\"en\")\n",
        "    return \";\".join(sorted(set(langs))) or \"en\"\n",
        "\n",
        "\n",
        "def extract_detail(url: str) -> dict:\n",
        "    html = fetch(url)\n",
        "    if not html:\n",
        "        return {\"title\":\"N/A\",\"description\":\"N/A\",\"publication_date\":\"\",\"deadline\":\"\",\"doc_url\":\"\",\"pdf_desc\":\"\",\"pdfs\":[],\"lang\":\"en\",\"detail_url\":url}\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # Prefer main content/article container\n",
        "    content = soup.find(\"article\") or soup.select_one(\"main .node, .node__content, .region-content, #content, main\") or soup\n",
        "\n",
        "    # Title: h1 within content; fallback to og:title; ensure not navigation placeholder\n",
        "    title = \"N/A\"\n",
        "    h1 = content.find(\"h1\") if content else None\n",
        "    if h1:\n",
        "        t = h1.get_text(\" \", strip=True)\n",
        "        if t and t.lower() not in [\"main navigation\", \"navigation\"]:\n",
        "            title = t\n",
        "    if title == \"N/A\":\n",
        "        og = soup.find(\"meta\", attrs={\"property\":\"og:title\"})\n",
        "        if og and og.get(\"content\") and og[\"content\"].strip().lower() not in [\"main navigation\", \"navigation\"]:\n",
        "            title = og[\"content\"].strip()\n",
        "\n",
        "    # Description: paragraphs inside content, skip placeholders\n",
        "    description = \"N/A\"\n",
        "    if content:\n",
        "        paras = content.find_all(\"p\")\n",
        "        for p in paras:\n",
        "            txt = p.get_text(\" \", strip=True)\n",
        "            if not txt:\n",
        "                continue\n",
        "            low = txt.lower()\n",
        "            if any(bad in low for bad in [\n",
        "                \"select filters below\", \"cookies\", \"main navigation\", \"subscribe\", \"share this\", \"related content\"\n",
        "            ]):\n",
        "                continue\n",
        "            if len(txt) < 40:\n",
        "                continue\n",
        "            description = txt\n",
        "            break\n",
        "\n",
        "    body_text = content.get_text(\" \", strip=True) if content else soup.get_text(\" \", strip=True)\n",
        "\n",
        "    # Dates\n",
        "    publication_date = \"\"\n",
        "    for lab in [\"published\", \"publication date\", \"date of publication\", \"date\"]:\n",
        "        m = re.search(lab + r\"[^\\d]{0,80}(.*)$\", body_text, flags=re.I)\n",
        "        if m:\n",
        "            publication_date = norm_date(m.group(0)) or norm_date(m.group(1))\n",
        "            if publication_date: break\n",
        "    if not publication_date:\n",
        "        publication_date = norm_date(body_text)\n",
        "\n",
        "    deadline = \"\"\n",
        "    for lab in [\"deadline\", \"submission\", \"closing\", \"application deadline\", \"tenders must be submitted\"]:\n",
        "        m = re.search(lab + r\"[^\\d]{0,80}(.*)$\", body_text, flags=re.I)\n",
        "        if m:\n",
        "            deadline = norm_date(m.group(0)) or norm_date(m.group(1))\n",
        "            if deadline: break\n",
        "\n",
        "    # Tendering documents URL and description\n",
        "    doc_url = \"\"; pdf_desc = \"\"\n",
        "    for a in content.find_all(\"a\", href=True) if content else soup.find_all(\"a\", href=True):\n",
        "        txt = a.get_text(\" \", strip=True)\n",
        "        href = a[\"href\"].strip()\n",
        "        if any(k in txt.lower() for k in [\"tender document\",\"documentation\",\"technical specification\",\"annex\",\"download\",\"documents\",\"tender dossier\"]):\n",
        "            doc_url = urljoin(BASE, href)\n",
        "            parent_text = a.find_parent().get_text(\" \", strip=True) if a.find_parent() else txt\n",
        "            pdf_desc = parent_text[:250]\n",
        "            break\n",
        "\n",
        "    # Collect PDFs\n",
        "    pdfs = []\n",
        "    for a in (content.find_all(\"a\", href=True) if content else soup.find_all(\"a\", href=True)):\n",
        "        href = a[\"href\"].strip()\n",
        "        if href.lower().endswith((\".pdf\",\".xlsx\",\".xls\")):\n",
        "            pdfs.append(urljoin(BASE, href))\n",
        "\n",
        "    # Languages\n",
        "    lang = detect_languages(body_text)\n",
        "\n",
        "    # PDF snippet fallback\n",
        "    if (description == \"N/A\") and pdf_text and pdfs:\n",
        "        content_bytes = fetch(pdfs[0])\n",
        "        snippet = pdf_text(content_bytes)[:800] if content_bytes else \"\"\n",
        "        if snippet:\n",
        "            description = snippet\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"description\": description,\n",
        "        \"publication_date\": publication_date,\n",
        "        \"deadline\": deadline,\n",
        "        \"doc_url\": doc_url,\n",
        "        \"pdf_desc\": pdf_desc,\n",
        "        \"pdfs\": sorted(set(pdfs)),\n",
        "        \"lang\": lang,\n",
        "        \"detail_url\": url\n",
        "    }\n",
        "\n",
        "\n",
        "rows = []\n",
        "visited = set()\n",
        "page_num = 1\n",
        "more_pages = True\n",
        "\n",
        "# Logging helper\n",
        "\n",
        "def log(msg: str):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# Helper: find listing links reliably on listing pages\n",
        "\n",
        "def find_listing_links(soup: BeautifulSoup) -> list[tuple[str,str]]:\n",
        "    main = soup.find(\"main\") or soup\n",
        "    links = []\n",
        "    # Target ONLY detail pages under /engage/procurements/<slug>\n",
        "    for a in main.find_all(\"a\", href=True):\n",
        "        href = (a[\"href\"] or \"\").strip()\n",
        "        if not href or href.startswith(\"#\"): \n",
        "            continue\n",
        "        full = urljoin(BASE, href)\n",
        "        low = full.lower()\n",
        "        if not low.startswith(BASE + \"/\"): \n",
        "            continue\n",
        "        if any(x in low for x in [\"/contact\", \"/privacy\", \"/legal\", \"/cookies\", \"/media\"]): \n",
        "            continue\n",
        "        # Must be detail: path depth >= 4 (/engage/procurements/<slug>) and no query/hash\n",
        "        if \"/engage/procurements/\" in low and low.rstrip(\"/\").count(\"/\") >= 4 and not any(x in low for x in [\"?\", \"#\"]):\n",
        "            text = a.get_text(\" \", strip=True)\n",
        "            snippet = str(a)[:180].replace('\\n',' ')\n",
        "            log(f\"   link(detail): {text[:80]} -> {full} | a-snippet: {snippet}\")\n",
        "            links.append((full, text))\n",
        "    # Deduplicate order-preserving\n",
        "    seen = set(); out = []\n",
        "    for u,t in links:\n",
        "        if u in seen: \n",
        "            continue\n",
        "        seen.add(u); out.append((u,t))\n",
        "    return out\n",
        "\n",
        "# Helper: resolve listing/taxonomy pages to detail pages by scanning their internal links\n",
        "\n",
        "def resolve_to_detail(url: str) -> str:\n",
        "    low = url.lower()\n",
        "    if \"/engage/procurements/\" in low and low.rstrip(\"/\").count(\"/\") >= 4 and not any(x in low for x in [\"?\", \"#\"]):\n",
        "        return url\n",
        "    html = fetch(url)\n",
        "    if not html:\n",
        "        return url\n",
        "    s = BeautifulSoup(html, \"html.parser\")\n",
        "    for full, text in find_listing_links(s):\n",
        "        low2 = full.lower()\n",
        "        if \"/engage/procurements/\" in low2 and low2.rstrip(\"/\").count(\"/\") >= 4 and not any(x in low2 for x in [\"?\", \"#\"]):\n",
        "            return full\n",
        "    return url\n",
        "\n",
        "while more_pages and page_num <= MAX_PAGES:\n",
        "    page_url = URL if page_num == 1 else f\"{URL}?page={page_num-1}\"\n",
        "    log(f\"== Page {page_num} URL: {page_url}\")\n",
        "    html = fetch(page_url)\n",
        "    soup = BeautifulSoup(html, \"html.parser\") if html else BeautifulSoup(\"\", \"html.parser\")\n",
        "\n",
        "    items = find_listing_links(soup)\n",
        "    log(f\"Page {page_num}: {len(items)} candidate links (listing)\")\n",
        "\n",
        "    kept = 0\n",
        "    page_has_old = False\n",
        "    for url_found, anchor_text in items:\n",
        "        detail_url = resolve_to_detail(url_found)\n",
        "        if detail_url in visited: \n",
        "            continue\n",
        "        visited.add(detail_url)\n",
        "        if detail_url.lower().endswith((\".pdf\",\".xls\",\".xlsx\")):\n",
        "            continue\n",
        "        log(f\" -> Detail: {detail_url}\")\n",
        "        det = extract_detail(detail_url)\n",
        "        # Fallback: use anchor text as title if detail page yielded N/A\n",
        "        if (not det.get(\"title\") or det.get(\"title\") == \"N/A\") and (anchor_text and anchor_text.strip() and anchor_text.strip().lower() not in [\"read more\", \"more\", \"details\"]):\n",
        "            det[\"title\"] = anchor_text.strip()\n",
        "            log(f\"    title fallback from anchor: {det['title'][:100]}\")\n",
        "        else:\n",
        "            log(f\"    title: {det.get('title','N/A')[:100]}\")\n",
        "        if det.get(\"title\") == \"N/A\" or det.get(\"description\") == \"N/A\":\n",
        "            log(f\"    Warn: N/A fields (title/description) for {detail_url}\")\n",
        "        dt = parse_date_any(det.get(\"publication_date\",\"\")) or parse_date_any(det.get(\"deadline\",\"\"))\n",
        "        if dt and dt < CUTOFF:\n",
        "            page_has_old = True\n",
        "        rows.append(det)\n",
        "        kept += 1\n",
        "\n",
        "    log(f\"Page {page_num}: kept {kept} tenders\")\n",
        "\n",
        "    next_link = soup.find(\"a\", string=lambda s: isinstance(s,str) and s.strip().lower() in [\"next\",\"older\",\"more\",\">\"])\n",
        "    page_num += 1 if next_link else 0\n",
        "    more_pages = bool(next_link)\n",
        "\n",
        "# Build structured output rows\n",
        "out_rows = []\n",
        "for r in rows:\n",
        "    out_rows.append({\n",
        "        \"organization\": \"ESPON\",\n",
        "        \"languages\": r.get(\"lang\",\"en\"),\n",
        "        \"submission_language\": r.get(\"lang\",\"en\"),\n",
        "        \"deadline\": r.get(\"deadline\",\"\"),\n",
        "        \"Round of questions 1\": \"N/A\",\n",
        "        \"Submit by\": \"N/A\",\n",
        "        \"cpv_code\": \"N/A\",\n",
        "        \"cpv_label\": \"N/A\",\n",
        "        \"title\": r.get(\"title\",\"\"),\n",
        "        \"publication_id\": \"N/A\",\n",
        "        \"publication_date\": r.get(\"publication_date\",\"\"),\n",
        "        \"url\": r.get(\"detail_url\",\"\"),\n",
        "        \"Description\": r.get(\"description\",\"\"),\n",
        "        \"pdf_description\": r.get(\"pdf_desc\",\"\"),\n",
        "        \"pdf_urls\": \";\".join(r.get(\"pdfs\", [])[:12])\n",
        "    })\n",
        "\n",
        "cols = [\n",
        "    \"organization\",\"languages\",\"submission_language\",\"deadline\",\"Round of questions 1\",\"Submit by\",\n",
        "    \"cpv_code\",\"cpv_label\",\"title\",\"publication_id\",\"publication_date\",\"url\",\"Description\",\"pdf_description\",\"pdf_urls\"\n",
        "]\n",
        "df = pd.DataFrame(out_rows, columns=cols)\n",
        "df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Wrote {len(df)} rows to {OUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting historical sweep (includes closed if listed in pagination)...\n",
            "== Hist Page 1 URL: https://www.espon.eu/engage/procurements\n",
            "   GET: https://www.espon.eu/engage/procurements\n",
            "   link(detail): Open PIN - [WEBHOUSE] An ESPON barometer on European local housing markets Annou -> https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <div cl\n",
            "   link(detail): Closed Prior Information Notice (PIN) for 4 Targeted Analysis Projects Announcem -> https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <div clas\n",
            "   link(detail): Open Call for expression of interest to establish a pool of editors to develop c -> https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\"> <div class=\"views-row\"> <div class=\"engage-status-wrapper\"> <di\n",
            "   link(detail): Open [SECC] - Spill-over effects of European capitals of culture on territorial  -> https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development | a-snippet: <a class=\"link-to-node\" href=\"/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\"> <div class=\"views-row\"> <div class=\"engage-status-wra\n",
            "Hist Page 1: 4 candidate links\n",
            " -> Hist Detail: https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\n",
            "   GET: https://www.espon.eu/engage/procurements/pin-webhouse-espon-barometer-european-local-housing-markets\n",
            "    title fallback from anchor: Open PIN - [WEBHOUSE] An ESPON barometer on European local housing markets Announcements 05 June 203\n",
            " -> Hist Detail: https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\n",
            "   GET: https://www.espon.eu/engage/procurements/prior-information-notice-pin-4-targeted-analysis-projects\n",
            "    title fallback from anchor: Closed Prior Information Notice (PIN) for 4 Targeted Analysis Projects Announcements 02 June 2030\n",
            " -> Hist Detail: https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\n",
            "   GET: https://www.espon.eu/engage/procurements/call-expression-interest-establish-pool-editors-develop-content\n",
            "    title fallback from anchor: Open Call for expression of interest to establish a pool of editors to develop content Calls 20 Sept\n",
            " -> Hist Detail: https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\n",
            "   GET: https://www.espon.eu/engage/procurements/secc-spill-over-effects-european-capitals-culture-territorial-development\n",
            "    title fallback from anchor: Open [SECC] - Spill-over effects of European capitals of culture on territorial development Calls 21\n",
            "Hist Page 1: kept 4 tenders\n",
            "Historical sweep wrote 4 rows to /Users/cedricjansen/Desktop/bak-economics/Cedric/espon_tenders_all.csv\n"
          ]
        }
      ],
      "source": [
        "# Multi-filter sweep to capture open + closed tenders\n",
        "# Reuses helpers defined above: log, fetch, find_listing_links, resolve_to_detail, extract_detail, parse_date_any\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "OUT_ALL = \"/Users/cedricjansen/Desktop/bak-economics/Cedric/espon_tenders_all.csv\"\n",
        "CUTOFF_HIST = datetime(2018, 1, 1)\n",
        "MAX_PAGES_PER_FILTER = 20\n",
        "\n",
        "# Try different filter combinations to get both open and closed\n",
        "FILTERS = [\n",
        "    \"\",  # default (open only)\n",
        "    \"?f[0]=status%3AClosed\",  # closed only\n",
        "    \"?f[0]=status%3AOpen\",    # open only (explicit)\n",
        "    \"?f[0]=status%3AOpen&f[1]=status%3AClosed\",  # both open and closed\n",
        "]\n",
        "\n",
        "rows_h = []\n",
        "visited_h = set()\n",
        "\n",
        "log(\"Starting multi-filter sweep (open + closed)...\")\n",
        "\n",
        "for filter_idx, filter_suffix in enumerate(FILTERS):\n",
        "    log(f\"=== Filter {filter_idx + 1}/{len(FILTERS)}: {filter_suffix or 'default'}\")\n",
        "    \n",
        "    page_num_h = 1\n",
        "    more_pages_h = True\n",
        "    \n",
        "    while more_pages_h and page_num_h <= MAX_PAGES_PER_FILTER:\n",
        "        page_url = f\"{URL}{filter_suffix}\" if page_num_h == 1 else f\"{URL}{filter_suffix}&page={page_num_h-1}\"\n",
        "        log(f\"== Filter {filter_idx + 1} Page {page_num_h} URL: {page_url}\")\n",
        "        html = fetch(page_url)\n",
        "        soup = BeautifulSoup(html, \"html.parser\") if html else BeautifulSoup(\"\", \"html.parser\")\n",
        "\n",
        "        items = find_listing_links(soup)\n",
        "        log(f\"Filter {filter_idx + 1} Page {page_num_h}: {len(items)} candidate links\")\n",
        "\n",
        "        kept = 0\n",
        "        page_has_old = False\n",
        "        for url_found, anchor_text in items:\n",
        "            detail_url = resolve_to_detail(url_found)\n",
        "            if detail_url in visited_h:\n",
        "                continue\n",
        "            visited_h.add(detail_url)\n",
        "            if detail_url.lower().endswith((\".pdf\",\".xls\",\".xlsx\")):\n",
        "                continue\n",
        "            log(f\" -> Detail: {detail_url}\")\n",
        "            det = extract_detail(detail_url)\n",
        "            if (not det.get(\"title\") or det.get(\"title\") == \"N/A\") and (anchor_text and anchor_text.strip() and anchor_text.strip().lower() not in [\"read more\", \"more\", \"details\"]):\n",
        "                det[\"title\"] = anchor_text.strip()\n",
        "                log(f\"    title fallback from anchor: {det['title'][:100]}\")\n",
        "            dt = parse_date_any(det.get(\"publication_date\",\"\")) or parse_date_any(det.get(\"deadline\",\"\"))\n",
        "            if dt and dt < CUTOFF_HIST:\n",
        "                page_has_old = True\n",
        "            rows_h.append(det)\n",
        "            kept += 1\n",
        "\n",
        "        log(f\"Filter {filter_idx + 1} Page {page_num_h}: kept {kept} tenders\")\n",
        "\n",
        "        next_link = soup.find(\"a\", string=lambda s: isinstance(s,str) and s.strip().lower() in [\"next\",\"older\",\"more\",\">\"])\n",
        "        page_num_h += 1 if next_link else 0\n",
        "        more_pages_h = bool(next_link and not page_has_old)\n",
        "        \n",
        "        # Stop if no more pages for this filter\n",
        "        if not next_link:\n",
        "            log(f\"Filter {filter_idx + 1}: no more pages\")\n",
        "            break\n",
        "\n",
        "# Build structured output and save\n",
        "out_rows_h = []\n",
        "for r in rows_h:\n",
        "    out_rows_h.append({\n",
        "        \"organization\": \"ESPON\",\n",
        "        \"languages\": r.get(\"lang\",\"en\"),\n",
        "        \"submission_language\": r.get(\"lang\",\"en\"),\n",
        "        \"deadline\": r.get(\"deadline\",\"\"),\n",
        "        \"Round of questions 1\": \"N/A\",\n",
        "        \"Submit by\": \"N/A\",\n",
        "        \"cpv_code\": \"N/A\",\n",
        "        \"cpv_label\": \"N/A\",\n",
        "        \"title\": r.get(\"title\",\"\"),\n",
        "        \"publication_id\": \"N/A\",\n",
        "        \"publication_date\": r.get(\"publication_date\",\"\"),\n",
        "        \"url\": r.get(\"detail_url\",\"\"),\n",
        "        \"Description\": r.get(\"description\",\"\"),\n",
        "        \"pdf_description\": r.get(\"pdf_desc\",\"\"),\n",
        "        \"pdf_urls\": \";\".join(r.get(\"pdfs\", [])[:12])\n",
        "    })\n",
        "\n",
        "cols = [\n",
        "    \"organization\",\"languages\",\"submission_language\",\"deadline\",\"Round of questions 1\",\"Submit by\",\n",
        "    \"cpv_code\",\"cpv_label\",\"title\",\"publication_id\",\"publication_date\",\"url\",\"Description\",\"pdf_description\",\"pdf_urls\"\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "df_all = pd.DataFrame(out_rows_h, columns=cols).drop_duplicates(subset=[\"url\"]) if out_rows_h else pd.DataFrame(columns=cols)\n",
        "\n",
        "df_all.to_csv(OUT_ALL, index=False, encoding=\"utf-8-sig\")\n",
        "log(f\"Historical sweep wrote {len(df_all)} rows to {OUT_ALL}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
